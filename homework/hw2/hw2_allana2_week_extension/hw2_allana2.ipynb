{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Gridworld Action Space Restriction\n",
    "There is a problem, potentially, since staying in a location is not an allowable action (technically) but an agent could repeatedly choose an invalid action (if the invalid action is selectable in the first place). So I need to further prune the set of actions, depending on the state, but I do not want to do so in a way that is manual.\n",
    "\n",
    "The way to do this is to count the number of non-corner edge states and account for that in the allowable set of actions. Since the area of the grid is length^2, the perimeter is 4$*$length, and the number of non-corner edge states is therefore 4$*$(length-1) and the number of corners is 4. Therefore, the number of allowable actions (dependent on state) is: $\\left(4*{length}^2-4*length\\right)*4+4*(length-1)*3+4*2$ so for $length=5$ we have $(100-20)*4+16*3+8=320+48+8=376$ actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2],[3,0]])\n",
    "print(np.flipud(a))\n",
    "np.array([[-1,-1,-1,-1,-1,-1,-1],[-1, 0, 10, 0, 5, 0, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "class gridworld(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self): \n",
    "    self.gridnum = int(7) #size of gridworld\n",
    "    self.location_x = np.random.randint(int(1),self.gridnum)\n",
    "    self.location_y = np.random.randint(int(1),self.gridnum)\n",
    "    self.action = int(0) # no initial action until computed\n",
    "    self.previous_action = self.action\n",
    "    self.previous_x = self.location_x\n",
    "    self.previous_y = self.location_y\n",
    "    self.previous_previous_x = self.previous_x\n",
    "    self.previous_previous_y = self.previous_y\n",
    "    self.allowed_actions = np.array([int(0)])\n",
    "    self.actionSet = np.matrix([1,2,3,4])\n",
    "    self.episode_length = int(100)\n",
    "    self.num_episodes = int(10000) # currently only running one episode\n",
    "    self.my_alpha = 0.1\n",
    "    self.my_gamma = 0.9\n",
    "    self.my_epsilon = 0.1\n",
    "    self.my_reward =np.array([ [-1,-1,-1,-1,-1,-1,-1],[-1, 0, 10, 0, 5, 0, -1],[-1, 0, 0, 0, 0, 0, -1],\\\n",
    "    [-1, 0, 0, 0, 0, 0, -1],[-1, 0, 0, 0, 0, 0, -1],[-1, 0, 0, 0, 0, 0, -1],[-1, -1, -1, -1, -1, -1, -1] ]) \n",
    "    self.my_reward = np.flipud(self.my_reward)\n",
    "    self.my_reward_model = np.zeros([self.gridnum,self.gridnum])#reward model updated based on observations\n",
    "    self.my_q_function = np.random.rand(self.gridnum,self.gridnum, int(4))# randomly initialized via reward model\n",
    "    self.my_reward_log = np.random.rand(1, self.episode_length*self.num_episodes) # used to store reward for each time step\n",
    "    self.my_episodic_cumulative_reward_log = np.random.rand(1,self.num_episodes)\n",
    "    pass\n",
    "  def get_allowed_actions(self):\n",
    "    if self.location_x == 0 and self.location_y == 0:\n",
    "        self.allowed_actions = np.array([1,4])[np.newaxis]\n",
    "    elif self.location_x == 0 and self.location_y == self.gridnum - 1:\n",
    "        self.allowed_actions = np.array([2,4])[np.newaxis]\n",
    "    elif self.location_x == self.gridnum - 1 and self.location_y == 0:\n",
    "        self.allowed_actions = np.array([1,3])[np.newaxis]\n",
    "    elif self.location_x == self.gridnum - 1 and self.location_y == self.gridnum - 1:\n",
    "        self.allowed_actions = np.array([2,3])[np.newaxis]\n",
    "    elif self.location_x == 0:\n",
    "        self.allowed_actions = np.array([1,2,4])[np.newaxis]\n",
    "    elif self.location_x == self.gridnum - 1:\n",
    "        self.allowed_actions = np.array([1,2,3])[np.newaxis]\n",
    "    elif self.location_y == 0:\n",
    "        self.allowed_actions = np.array([1,3,4])[np.newaxis]\n",
    "    elif self.location_y == self.gridnum - 1:\n",
    "        self.allowed_actions = np.array([2,3,4])[np.newaxis]\n",
    "    else:\n",
    "        self.allowed_actions = np.array([1,2,3,4])[np.newaxis]\n",
    "    return self.allowed_actions \n",
    "  def my_policy(self):\n",
    "    if self.previous_x != self.location_x and self.previous_y != self.location_y:\n",
    "        self.previous_action = self.action # only accurate for second step in episode\n",
    "    self.action = self.allowed_actions[0,\\\n",
    "    np.argmax(self.my_q_function[self.location_x,self.location_y,self.allowed_actions-1])]\n",
    "    if np.random.rand() <= self.my_epsilon:\n",
    "        self.action = self.allowed_actions[0,np.random.randint(0,self.allowed_actions.shape[1])]\n",
    "    pass\n",
    "  def render(self,render_label): #mode='human', close=False <- no idea what this is for\n",
    "    if render_label == 't':\n",
    "        ax = fig.gca()    \n",
    "        ax.clear()\n",
    "        ax.grid(which='major', axis='both', linestyle='-')\n",
    "        circle2 = plt.Circle((world.location_x-0.5, world.location_y-0.5), 0.5, color='blue')#rand initialization\n",
    "        ax.add_artist(circle2)\n",
    "        fig.canvas.draw()\n",
    "    pass\n",
    "  def render_init(self,render_label):\n",
    "    if render_label == 't':\n",
    "        intervals = float(1/world.gridnum)# dimension of grid affects size\n",
    "        loc = plticker.MultipleLocator(base=intervals)\n",
    "        ax.xaxis.set_major_locator(loc)\n",
    "        ax.set_xlim(0, world.gridnum)\n",
    "        ax.yaxis.set_major_locator(loc)\n",
    "        ax.set_ylim(0, world.gridnum)\n",
    "    pass\n",
    "  def reset(self):\n",
    "    self.location_x = np.random.randint(int(1),self.gridnum)\n",
    "    self.location_y = np.random.randint(int(1),self.gridnum)\n",
    "    self.previous_x = self.location_x\n",
    "    self.previous_y = self.location_y\n",
    "    pass\n",
    "  def step(self):\n",
    "    self.previous_previous_x = self.previous_x\n",
    "    self.previous_previous_y = self.previous_y\n",
    "    self.previous_x = self.location_x# only accurate for second step in episode\n",
    "    self.previous_y = self.location_y# only accurate for second step in episode\n",
    "    desired_action = self.action\n",
    "    if np.random.rand() <= 0.1: # this part of the method is to enforce a 10% chance of a random transition\n",
    "        self.action = self.allowed_actions[0,np.random.randint(1,self.allowed_actions.shape[1])]\n",
    "    if self.action == 1: # this part of the method is to select the desired deterministic action\n",
    "        self.location_y += 1\n",
    "    elif self.action == 2:\n",
    "        self.location_y += -1\n",
    "    elif self.action == 3:\n",
    "        self.location_x += -1\n",
    "    elif self.action == 4:\n",
    "        self.location_x += 1\n",
    "    self.action = desired_action\n",
    "    pass #return [(self.location_x, self.location_y)]\n",
    "  def update_reward_model(self):\n",
    "    self.my_reward_model[self.location_x,self.location_y]\\\n",
    "    = self.my_reward[self.location_x,self.location_y]\n",
    "    pass\n",
    "  def update_my_q_function(self):#,action,location_x,location_y):\n",
    "    self.my_q_function[self.previous_x,self.previous_y,self.previous_action-1] +=\\\n",
    "    self.my_alpha*(self.my_reward_model[self.location_x,self.location_y]+\\\n",
    "    self.my_gamma*self.my_q_function[self.location_x,self.location_y,self.action-1]-\\\n",
    "    self.my_q_function[self.previous_x,self.previous_y,self.previous_action-1])\n",
    "    pass\n",
    "fig1, (ax1)=plt.subplots()\n",
    "fig, (ax)=plt.subplots()\n",
    "world = gridworld()\n",
    "render_label = 't'\n",
    "world.render_init(render_label)\n",
    "k=0 # counter for episodic cumulative reward\n",
    "for i in range(0,world.episode_length * world.num_episodes - 1):\n",
    "    world.update_reward_model()\n",
    "    world.my_reward_log[0,i] = world.my_reward[world.location_x,world.location_y]\n",
    "    world.update_my_q_function()\n",
    "    if world.my_reward[world.location_x,world.location_y] < 0:\n",
    "        world.location_x = world.previous_x\n",
    "        world.previous_x = world.previous_previous_x\n",
    "        world.location_y = world.previous_y\n",
    "        world.previous_y = world.previous_previous_y\n",
    "    elif world.my_reward_log[0,i] > 5:\n",
    "        world.location_x = 1+1\n",
    "        world.location_y = 0+1\n",
    "    elif world.my_reward_log[0,i] > 0:\n",
    "        world.location_x = 3+1\n",
    "        world.location_y = 2+1\n",
    "    world.allowed_actions = world.get_allowed_actions()\n",
    "    world.my_policy()\n",
    "    world.step()\n",
    "    world.render(render_label)\n",
    "    if np.mod(i+1,world.episode_length) == 0:\n",
    "        world.my_episodic_cumulative_reward_log[0,k] = \\\n",
    "        np.sum(world.my_reward_log[0,(k*world.episode_length):(i+1)])# sums from k*episode_length to i\n",
    "        if np.mod(i+1,np.floor(0.1*world.episode_length*world.num_episodes)-1) == 0:\n",
    "            print(np.floor(100*i/(world.episode_length*world.num_episodes)))\n",
    "        k += 1\n",
    "        world.reset()\n",
    "ax1.plot(world.my_episodic_cumulative_reward_log[0,0:-1])\n",
    "plt.xlabel('episode number')\n",
    "plt.ylabel('total episodic reward')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
