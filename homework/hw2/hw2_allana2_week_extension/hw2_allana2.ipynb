{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Gridworld Action Space Restriction\n",
    "There is a problem, potentially, since staying in a location is not an allowable action (technically) but an agent could repeatedly choose an invalid action (if the invalid action is selectable in the first place). So I need to further prune the set of actions, depending on the state, but I do not want to do so in a way that is manual.\n",
    "\n",
    "The way to do this is to count the number of non-corner edge states and account for that in the allowable set of actions. Since the area of the grid is length^2, the perimeter is 4$*$length, and the number of non-corner edge states is therefore 4$*$(length-1) and the number of corners is 4. Therefore, the number of allowable actions (dependent on state) is: $\\left(4*{length}^2-4*length\\right)*4+4*(length-1)*3+4*2$ so for $length=5$ we have $(100-20)*4+16*3+8=320+48+8=376$ actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "class gridworld(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self): \n",
    "    self.gridnum = int(5) #size of gridworld\n",
    "    self.location_x = np.random.randint(int(0),self.gridnum)+0.5\n",
    "    self.location_y = np.random.randint(int(0),self.gridnum)+0.5\n",
    "    self.action = int(0) # no initial action until computed\n",
    "    self.previous_action = self.action\n",
    "    self.previous_x = self.location_x\n",
    "    self.previous_y = self.location_y\n",
    "    self.actionSet = np.array([1,2,3,4])\n",
    "    self.episode_length = int(100)\n",
    "    self.num_episodes = int(10)\n",
    "    self.my_alpha = 0.3\n",
    "    self.my_gamma = 0.9\n",
    "    self.my_reward = np.matrix('3.3 8.8 4.4 5.4 1.5;1.5 3.0 2.3 1.9 0.5;0.1 0.7 0.7 0.4 -0.4; -1.0 -0.4 -0.4 -0.6 -1.2; -1.9 -1.3 -1.2 -1.4 -2.0') \n",
    "    self.my_reward_model = np.zeros([self.gridnum,self.gridnum])#reward model updated based on observations\n",
    "    self.my_q_function = np.random.rand(self.gridnum,self.gridnum, int(4))# randomly initialized via reward model\n",
    "    self.my_reward_log = np.random.rand(self.num_episodes, self.episode_length) # used to store reward for each time step\n",
    "    pass\n",
    "  def get_allowed_actions(self):\n",
    "    if self.location_x == 0.5 and self.location_y == 0.5:\n",
    "        allowed_actions = np.array([1,4])\n",
    "    elif self.location_x == 0.5 and self.location_y == self.gridnum - 0.5:\n",
    "        allowed_actions = np.array([2,4])\n",
    "    elif self.location_x == self.gridnum - 0.5 and self.location_y == 0.5:\n",
    "        allowed_actions = np.array([1,3])\n",
    "    elif self.location_x == self.gridnum - 0.5 and self.location_y == self.gridnum - 0.5:\n",
    "        allowed_actions = np.array([2,3])\n",
    "    elif self.location_x == 0.5:\n",
    "        allowed_actions = np.array([1,2,4])\n",
    "    elif self.location_x == self.gridnum - 0.5:\n",
    "        allowed_actions = np.array([1,2,3])\n",
    "    elif self.location_y == 0.5:\n",
    "        allowed_actions = np.array([1,3,4])\n",
    "    elif self.location_y == self.gridnum - 0.5:\n",
    "        allowed_actions = np.array([2,3,4])\n",
    "    else:\n",
    "        allowed_actions = np.array([1,2,3,4])\n",
    "    return allowed_actions\n",
    "  def my_policy(self):\n",
    "    self.previous_action = self.action\n",
    "    # next call available actions method, evaluate maximum q function and then assign index to appropriate self.action\n",
    "    pass\n",
    "  def step(self):\n",
    "    self.previous_x = self.location_x\n",
    "    self.previous_y = self.location_y\n",
    "    desired_action = self.action\n",
    "    if np.random.rand() <= 0.1: # this part of the method is to enforce a 10% chance of a random transition\n",
    "        self.action = int(np.random.randint(int(1),int(5)))\n",
    "    if self.action == 1: # this part of the method is to select the desired deterministic action\n",
    "        self.location_y += 1\n",
    "    elif self.action == 2:\n",
    "        self.location_y += -1\n",
    "    elif self.action == 3:\n",
    "        self.location_x += -1\n",
    "    elif self.action == 4:\n",
    "        self.location_x += 1\n",
    "    self.action = desired_action\n",
    "    pass #return [(self.location_x, self.location_y)]\n",
    "  def update_reward_model(self):\n",
    "    self.my_reward_model[self.location_x,self.location_y] = self.my_reward[self.location_x,location_y]\n",
    "    pass\n",
    "  def update_my_q_function(self):#,action,location_x,location_y):\n",
    "    self.my_q_function[self.previous_x,self.previous_y,self.previous_action-1] =\\\n",
    "    (1-self.alpha) * self.my_q_function[self.previous_x,self.previous_y,self.previous_action-1] +\\\n",
    "    self.alpha*(self.my_reward_model[self.location_x,self.location_y]+\\\n",
    "    self.gamma*self.my_q_function[self.location_x,self.location_y,self.best_action-1] )\n",
    "    pass\n",
    "  def reset(self):\n",
    "    pass\n",
    "  def render(self, mode='human', close=False):\n",
    "    pass\n",
    "\n",
    "world = gridworld()\n",
    "\n",
    "fig, (ax)=plt.subplots()#do not know alternative to command fig.canvas.draw() to have independent subplots shown\n",
    "fig1, (ax1)=plt.subplots()\n",
    "\n",
    "intervals = float(1/world.gridnum)# 5x5 grid, dimension of grid affects size\n",
    "loc = plticker.MultipleLocator(base=intervals)\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "ax.set_xlim(0, world.gridnum)\n",
    "ax.yaxis.set_major_locator(loc)\n",
    "ax.set_ylim(0, world.gridnum)\n",
    "\n",
    "ax.clear()\n",
    "ax.grid(which='major', axis='both', linestyle='-')\n",
    "circle2 = plt.Circle((world.location_x, world.location_y), 0.5, color='blue')#rand initialization\n",
    "ax = fig.gca()\n",
    "ax.add_artist(circle2)\n",
    "fig.canvas.draw()\n",
    "\n",
    "for i in range(0,world.episode_length - 1):\n",
    "    world.my_reward_log[0,i] = world.my_reward[(world.location_x,world.location_y)]\n",
    "#    world.my_q_function[world.location_x,world.location_y] = \\\n",
    "#    world.my_q_function[world.location_x,world.location_y] + world.my_alpha *\\\n",
    "#    (world.my_reward_log[0,i]*world.my_gamma*(??)-world.my_q_function[world.location_x,world.location_y])\n",
    "    ax.clear()\n",
    "    ax.grid(which='major', axis='both', linestyle='-')\n",
    "    my_allowed_actions = world.get_allowed_actions()\n",
    "    test = int(my_allowed_actions.shape[0]) # gets length of vector\n",
    "    test2 = np.random.randint(int(0),test)# generates random integer in range of vector\n",
    "    world.action = my_allowed_actions[test2]# selects random allowed action\n",
    "    world.step()\n",
    "    circle2 = plt.Circle((world.location_x, world.location_y), 0.5, color='blue')\n",
    "    ax = fig.gca()\n",
    "    ax.add_artist(circle2)\n",
    "    fig.canvas.draw()\n",
    "ax1.plot(np.cumsum(world.my_reward_log[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(int(5.1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
