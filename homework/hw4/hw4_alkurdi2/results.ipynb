{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vanilla Reinforce Algorithm**\n",
    "\n",
    "\n",
    "### Analytical derivative of softmax:\n",
    "\n",
    "<img src=\"Gradlogpolderiv.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Hard Vs Easy GW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing packages and setting environment\n",
    "from REINFORCE import REINFORCE \n",
    "import abdulgym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "env=abdulgym.make('AAgridworld-v0')\n",
    "\n",
    "### Initializing Policy & hyperparameters\n",
    "ep_len=100 #change to 100\n",
    "gamma=1\n",
    "learning_rate=0.001 #use 0.01\n",
    "num_epis_max=100000 #change to 10,000\n",
    "num_actions=env.action_space.n\n",
    "num_state=5*5\n",
    "\n",
    "theta=np.random.rand(num_actions,num_state)\n",
    "thetah=np.random.rand(num_actions,num_state)\n",
    "\n",
    "### Running Learning algorithm\n",
    "meta_data=[ep_len,gamma,learning_rate,num_epis_max,num_actions,num_state,theta]\n",
    "traj, reward_accum, theta_accum, action, state, _ , _= REINFORCE(env,meta_data)\n",
    "\n",
    "meta_datah=[ep_len,gamma,learning_rate,num_epis_max,num_actions,num_state,thetah]\n",
    "trajh, reward_accumh, theta_accumh, actionh, stateh, policyh, _= REINFORCE(env,meta_datah,difficulty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward=[]\n",
    "avg_rewardh=[]\n",
    "for i in range(num_epis_max):\n",
    "    if i > 100:\n",
    "        avg_reward.append(np.mean(reward_accum[i-100:]))\n",
    "        avg_rewardh.append(np.mean(reward_accumh[i-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Easy GW, episode length=\" ,ep_len, \"learning rate=\", learning_rate, \"gamma=\", gamma)\n",
    "plt.title('Accumulated reward per episode')\n",
    "plt.xlabel('time step')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.legend(['easy','hard'])\n",
    "plt.plot(range(1,num_epis_max+1), reward_accum, reward_accumh)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,num_epis_max-100), avg_reward, avg_rewardh)\n",
    "plt.title('Moving average reward')\n",
    "plt.legend(['easy','hard'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different policy parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section initializes theta to larger than [0,1] values\n",
    "theta2=np.random.rand(num_actions,num_state)*10 \n",
    "meta_data=[ep_len,gamma,learning_rate,num_epis_max,num_actions,num_state,theta2]\n",
    "traj2, reward_accum2, theta_accum2, action2, state2 , _ , _= REINFORCE(env,meta_data)\n",
    "avg_reward2=[]\n",
    "for i in range(num_epis_max):\n",
    "    if i > 100:\n",
    "        avg_reward2.append(np.mean(reward_accum2[i-100:]))\n",
    "                   \n",
    "plt.title(print(\"Hard, episode length=\" ,ep_len, \"learning rate=\", learning_rate, \"gamma=\", gamma))\n",
    "plt.xlabel('time step')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.plot(reward_accum2)\n",
    "plt.show()\n",
    "plt.plot(avg_reward2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE import softmax_policy\n",
    "[a,b]=theta.shape\n",
    "policy=np.zeros_like(theta)\n",
    "for i in range(b):\n",
    "    policy[:,i]=softmax_policy(theta[:,i])\n",
    "for i in range():\n",
    "    bigP[:,i]=softmax_policy(theta2[:,i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(-policy)\n",
    "plt.show()\n",
    "plt.imshow(-bigP)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE import softmax_policy\n",
    "x = np.linspace(0,6)\n",
    "y = np.linspace(0,6)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(5):\n",
    "    plt.annotate('', xy=(i+.5,-.5), xytext=(i+.5,4.5), arrowprops={'arrowstyle': '-'}, va='center')\n",
    "for i in range(5):\n",
    "    plt.annotate('', xy=(-0.5,i+.5), xytext=(4.5,i+.5), arrowprops={'arrowstyle': '-'}, va='center')\n",
    "\n",
    "# 0 right 1 left\n",
    "# 2 up    3 down    \n",
    "p=np.zeros(4)\n",
    "#p[0]=1 #probability of that action happening policy(action|state)\n",
    "#p[1]=1\n",
    "#p[2]=1\n",
    "#p[3]=1\n",
    "\n",
    "[a,b]=theta.shape\n",
    "policy=np.zeros_like(theta)\n",
    "for i in range(b):\n",
    "    policy[:,i]=softmax_policy(theta[:,i])\n",
    "policyh=np.zeros_like(theta)\n",
    "for i in range(b):\n",
    "    policyh[:,i]=softmax_policy(policy[:,i])\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        flat_loc=i*5+j #onehotting the state\n",
    "        #print(j,i)\n",
    "        #print(flat_loc)\n",
    "        p=policy[:,flat_loc]\n",
    "        \n",
    "        plt.annotate('', xytext=(i,j), xy=((.5*p[0]+i),j), arrowprops={'arrowstyle': '-|>'}, va='center')#right\n",
    "        plt.annotate('', xytext=(i,j), xy=(-(.5*p[1])+i,j), arrowprops={'arrowstyle': '-|>'}, va='center')#left\n",
    "        plt.annotate('', xytext=(i,j), xy=(i,0.5*p[2]+j), arrowprops={'arrowstyle': '-|>'}, va='center')#up\n",
    "        plt.annotate('', xytext=(i,j), xy=(i,-0.5*p[3]+j), arrowprops={'arrowstyle': '-|>'}, va='center')#down\n",
    "#plt.plot(x,y,'x')\n",
    "plt.xlim(-0.5,4.5)\n",
    "plt.ylim(-0.5,4.5)\n",
    "plt.title('Learned policy for easy GW')\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0,6)\n",
    "y = np.linspace(0,6)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(5):\n",
    "    plt.annotate('', xy=(i+.5,-.5), xytext=(i+.5,4.5), arrowprops={'arrowstyle': '-'}, va='center')\n",
    "for i in range(5):\n",
    "    plt.annotate('', xy=(-0.5,i+.5), xytext=(4.5,i+.5), arrowprops={'arrowstyle': '-'}, va='center')\n",
    "\n",
    "# 0 right 1 left\n",
    "# 2 up    3 down    \n",
    "ph=np.zeros(4)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        flat_loc=i*5+j #onehotting the state\n",
    "        #print(j,i)\n",
    "        #print(flat_loc)\n",
    "        ph=policyh[:,flat_loc]\n",
    "        \n",
    "        plt.annotate('', xytext=(i,j), xy=((.5*ph[0]+i),j), arrowprops={'arrowstyle': '-|>'}, va='center')#right\n",
    "        plt.annotate('', xytext=(i,j), xy=(-(.5*ph[1])+i,j), arrowprops={'arrowstyle': '-|>'}, va='center')#left\n",
    "        plt.annotate('', xytext=(i,j), xy=(i,0.5*ph[2]+j), arrowprops={'arrowstyle': '-|>'}, va='center')#up\n",
    "        plt.annotate('', xytext=(i,j), xy=(i,-0.5*ph[3]+j), arrowprops={'arrowstyle': '-|>'}, va='center')#down\n",
    "#plt.plot(x,y,'x')\n",
    "plt.xlim(-0.5,4.5)\n",
    "plt.title('Learned policy for hard GW')\n",
    "\n",
    "plt.ylim(-0.5,4.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The REINFORCE algorithm was written to learn a optimal policy for Gridworld environment. The hard GW example was harder to learn which resulted in noiser rewards, slower and more variance in learning.  \n",
    "\n",
    "Another variation tested was initialized theta as random values: [0,1) vs [0,10). That was brought up to test the effect of the exponent in the softmax function on results. It was discovered that learned policy had more gradient between highest and lowest probability as well as slightly faster learning curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
