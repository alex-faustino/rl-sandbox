{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "----Optimal Value function----\n",
      "[[21.97748529 24.4194281  21.97748529 18.4501845  16.60505978]\n",
      " [16.90176309 21.97748529 16.93027899 17.09563411 14.79990688]\n",
      " [17.14480769 19.77973676 17.34967131 11.98452209 13.27358837]\n",
      " [15.46117573 17.80176308 12.06506043 13.27617919 11.34411363]\n",
      " [10.5132031  16.02158677 12.95299324 11.47990883  8.88475738]]\n",
      "-----pi_star-----\n",
      "[[2. 0. 3. 2. 3.]\n",
      " [0. 0. 0. 3. 0.]\n",
      " [2. 0. 3. 1. 0.]\n",
      " [2. 0. 2. 0. 0.]\n",
      " [0. 0. 0. 3. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import gym_grid_world\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/Users/AmberS/miniconda3/lib/python3.5/site-packages')\n",
    "\n",
    "def main():\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    alpha = 0.9 # learning rate\n",
    "    epsilon = 0 # epsilon greedy policy\n",
    "    gamma = 0.9 # discount factor\n",
    "    env = gym.make('grid_world-v0')\n",
    "    T = 26\n",
    "    M = env.action_space.n\n",
    "    print(M)\n",
    "    N = env.state_space.n\n",
    "    k = 0\n",
    "    d = {}\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            d.update([((i,j),k)])\n",
    "            k+=1\n",
    "    # 'N' denotes number of states\n",
    "    # 'M' denotes number of actions\n",
    "    # 'T' denotes the terminal state\n",
    "    Q = np.zeros((N*N,M)) # initializing the Q matrix \n",
    "    tot_epi = 200 # denotes the maximum number of episodes \n",
    "    choice = [1,2] # for implementing epsilon-greedy policy\n",
    "    dist = [epsilon, 1-epsilon]\n",
    "    \n",
    "    for ep in range(tot_epi):\n",
    "        #s = np.random.randint(1,N*N) # randomly select initial state for each episode\n",
    "        env.reset()\n",
    "        s1,s2 = env.state\n",
    "        #print(s1-1,s2-1)\n",
    "        #print(d)\n",
    "        #s = d[(s1-1,s2-1)]\n",
    "        s = d[(s1,s2)]\n",
    "        itr = 0\n",
    "        ep_choice = random.choices(choice, dist)\n",
    "        if ep_choice == 2:\n",
    "            a = np.argmax(Q[s,:]) # this is not epsilon greedy\n",
    "        else:\n",
    "            a = np.random.randint(0,M)\n",
    "        while itr < 30000:\n",
    "            obs, reward, done, info = env.step(a)\n",
    "            s1,s2 = obs\n",
    "            #s_pr = d[(s1-1,s2-1)]\n",
    "            s_pr = d[(s1,s2)]\n",
    "            ep_choice = random.choices(choice, dist)\n",
    "            if ep_choice == 2:\n",
    "                a_pr = np.argmax(Q[s_pr,:]) # this is not epsilon greedy\n",
    "            else:\n",
    "                a_pr = np.random.randint(0,M)\n",
    "            #print(ep_choice)\n",
    "            #print(a)\n",
    "            \n",
    "            #print(reward)\n",
    "            \n",
    "            a_pr = np.argmax(Q[s,:])\n",
    "            Q[s,a] = Q[s,a] + alpha*(reward + gamma*Q[s_pr,a_pr] - Q[s,a])\n",
    "            s = s_pr\n",
    "            a = a_pr\n",
    "            itr +=1\n",
    "        epsilon = epsilon*0.9\n",
    "        \n",
    "    v_star = np.zeros((N,N))\n",
    "    pi_star = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            s = d[(i,j)]\n",
    "            f = np.amax(Q[s,:])\n",
    "            f1 = np.argmax(Q[s,:])\n",
    "            v_star[4-i,j] = f\n",
    "            pi_star[4-i,j] = f1\n",
    "    '''\n",
    "    v_star = np.amax(Q,axis=1)\n",
    "    \n",
    "    v_star_i = v_star[::-1]\n",
    "    v_star = np.reshape(v_star_i,(5,5))\n",
    "    pi_star = np.argmax(Q,axis=1)\n",
    "    pi_star_i = pi_star[::-1]\n",
    "    pi_star = np.reshape(pi_star_i,(5,5))\n",
    "    '''\n",
    "    print('----Optimal Value function----')\n",
    "    print(v_star)\n",
    "    #print(v_star_i)\n",
    "    print('-----pi_star-----')\n",
    "    print(pi_star)\n",
    "    \n",
    "    #print(Q)\n",
    "    \n",
    "\n",
    "main() # invoking the main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guide to pi_star:\n",
    "Value:0----->Action Up\n",
    "Value:1----->Action Down\n",
    "Value:2----->Action Right\n",
    "Value:3----->Action Left"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
