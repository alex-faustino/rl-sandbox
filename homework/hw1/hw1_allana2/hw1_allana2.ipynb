{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Reader(s)\n",
    "The below verifies that openai gym is properly installed. \n",
    "\n",
    "Note that one has to **restart kernel and clear outputs** to get rid of openai gym windows on my computer.\n",
    "\n",
    "## Initializing Environments\n",
    "### CartPole V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Acrobot V1 Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Environments Officially in OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print (envs.registry.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress in New Environment Implementations\n",
    "I have been able to create a pip install environment, following the instructions [here](https://github.com/openai/gym/tree/master/gym/envs), however I could not get it to run. I have attempted and failed to troubleshoot as proposed [here](https://github.com/openai/gym/issues/626). \n",
    "\n",
    "I have successfully installed but failed to implement [this gridworld for open-ai gym](https://github.com/xinleipan/gym-gridworld), which has an error addressed [here](https://github.com/openai/gym/issues/860) whose solution did not work for me, or escapes my intuition. \n",
    "\n",
    "As a test, I tried installing and running the ['recommended'](https://github.com/openai/gym/tree/master/gym/envs#how-to-create-new-environments-for-gym) environment [gym-soccer](https://github.comopenai/gym-soccer), and while the installation was successful (as was the case for the gridworld) I have not managed to run it.\n",
    "\n",
    "\n",
    "\n",
    "This stems from disappointingly unhelpful sources saying [\"It is quite simple\"](https://www.quora.com/How-does-one-create-a-custom-environment-utilising-OpenAIs-universe-gym-libraries). There are no available video tutorials to fill in the gaps, to the best of my knowledge. Video tutorials are of the form of [\"look at this staple problem, we will show you how to get a starting solution\"](https://www.youtube.com/results?search_query=openai+gym+tutorial) rather than \"let's make a new environment and run it in open ai gym.\"\n",
    "\n",
    "I didn't change anything except trying to run other files, but now gridworld seems promising. The environment does generate graphical output, in the form of a graph, but only one frame is generated, with a red square, green square and obstacles. I am getting a \"NotImplementedError\" in matplotlib, itself. If I use %matplotlib instead of \"%matplotlib notebook\" (without quotes) then the red square initalizes in different adjacent locations to coordinate 20,20. With this, I conclude that the gridworld environment has been implemented to some extent. The environment file is gridworld_env.py in hw1_allana2/gym-gridworld/gym_gridworld/envs however I do not know exactly how I set this up successfully.\n",
    "\n",
    "**Gridworld worked after resetting computer!** So that's something. Changes still required\n",
    "* <s>Agent movement</s>\n",
    "* <s>Obstacle Removal</s>\n",
    "* <s>Transition Model</s>\n",
    "* <s>Reward Model</s>\n",
    "* Random agent initialization\n",
    "\n",
    "## Agent Movement\n",
    "This is the easiest part. I copied the for loop from the Initializing Environments Section of this report over to the gridworld and now the red box moves around randomly.\n",
    "\n",
    "I have removed obstacles by making the file 'plain10.txt' which is a 5 by 5 gridworld surrounded by barriers. The agent randomly moves, and when it hits the goal location it resets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Model\n",
    "action = int(action) #  10% chance of uniformly random action\n",
    "if np.random.rand() <= 0.1:\n",
    "    action = int(np.random.randint(0,4))\n",
    "Note, due to action and actions being defined in other methods, I have just copied the code that I have added rather than making a self-contained operational script as an example.    \n",
    "### Reward Model\n",
    "        self.reward = np.matrix('0 0 0 0 0 0 0;0 3.3 8.8 4.4 5.4 1.5 0;0 1.5 3.0 2.3 1.9 0.5 0;0 0.1 0.7 0.7 0.4 -0.4 0; 0 -1.0 -0.4 -0.4 -0.6 -1.2 0;0 -1.9 -1.3 -1.2 -1.4 -2.0 0;0 0 0 0 0 0 0') \n",
    "### Initialization\n",
    "Currently, initialization is established by reading a text file called 'plain10.txt' and this establishes whether or not a state is the starting state of the agent or target. I still need to code a workaround for this. Otherwise, this environment is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/floppsy/anaconda3/lib/python3.5/site-packages/matplotlib/backend_bases.py:2437: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym_gridworld\n",
    "env = gym.make('gridworld-v0')\n",
    "env.verbose = True\n",
    "_ = env.reset()\n",
    "_ = env.step(env.action_space.sample())\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Commentary\n",
    "It's inefficient, because it was built in a the space of a busy week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What I Was Supposed To Do\n",
    "\n",
    "## 1) Gridworld\n",
    "[Assignment instructions](https://github.com/compdyn/598rl/wiki/HW1#what-to-do) state to implement Example 3.5 from [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "### Assumptions Allowed/Required\n",
    "* Agent is initialized in a uniformly random location in the gridworld for each trial (monte carlo?).\n",
    "\n",
    "## 2) Classic Control System\n",
    "[Assignment instructions](https://github.com/compdyn/598rl/wiki/HW1#what-to-do) state to implement **acrobot**, and to reference equations of motion in Chapter 3 of [Underactuated Robotics.](http://underactuated.mit.edu/underactuated.html)\n",
    "\n",
    "## 3) Custom Environment That Suits My Interest\n",
    "Since there isn't much guidance in the [assignment instructions] (creativity mandates this)(https://github.com/compdyn/598rl/wiki/HW1#what-to-do), candidate ideas for this will be listed and discussed below.\n",
    "### Modeling a Grid-World as a Series of Bandit Problems\n",
    "Assumptions that are required include:\n",
    "* the environment is stationary in terms of its statistics\n",
    "* that the value function has to be bounded in order for policies like the Upper Confidence Bound \n",
    "\n",
    "Note that martingales may be used to model systems with limited stochasticity.\n",
    "\n",
    "### Question for Reader(s)\n",
    "Would such a formulation of the problem constitute a new environment? My preliminary thought is \"no,\" but my optimism forces this to remain present on the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failed Implementations of New Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "env = gym.make('MyEnv-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
