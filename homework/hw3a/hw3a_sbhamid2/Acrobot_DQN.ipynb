{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from acrobot.acrobot import AcroEnvNew\n",
    "\n",
    "env = gym.make('MyAcrobot-v2').unwrapped\n",
    "device = torch.device(\"cpu\") ## Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Architecture of the Deep Q-neural network \n",
    "### This sample neural network architecture has been referenced from \n",
    "### https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the image-> current/previous for states\n",
    "screen_width = 600\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = 2.4*2 #env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_image():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:,238:398]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location>340: \n",
    "        cart_location = 340\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initializing thte policy and target neural networks\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "### Defining the optimizer as RMSprop\n",
    "optimizer = torch.optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "## Number of training samples to choose from at any instant\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "### get action state based on the e-greedy algorithm\n",
    "EPS = 0.1\n",
    "#steps_done = 0\n",
    "def get_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    #eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    #steps_done += 1\n",
    "    if sample > EPS: #eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(3)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This training model is referenced from the blog on reinforcement learning for cartpole \n",
    "### Reference: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "### Comments are added by me and the code has been thoroughly analyzed\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "def train_model():\n",
    "    ### If length of memory is less than batch_size then we skip the optimization it we have enough samples\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    ### Else we randomly choose a BTACH_SIZE number of samples \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute the policy Q(s, a), thereafter we select the corresponding action items\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute next state values\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # Compute the expected Q values for the next state\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    ### Evaluate objective function based on huber loss-> \n",
    "    ### acts as mean squared loss, i.e., (\\delta)^2/2 if |\\delta|<1 \n",
    "    ### acts as mean absolute loss, i.e., |\\delta|-0.5 elsewhere\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    ### Optimize the model using SGD implementation -> especially RMSprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    ### Update the training model based on the optimization carried out\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_no:  0\n",
      "episode_done:  True\n",
      "episode_no:  1\n",
      "episode_done:  True\n",
      "episode_no:  2\n",
      "episode_done:  True\n",
      "episode_no:  3\n",
      "episode_done:  True\n",
      "episode_no:  4\n",
      "episode_done:  True\n",
      "episode_no:  5\n",
      "episode_done:  True\n",
      "episode_no:  6\n",
      "episode_done:  True\n",
      "episode_no:  7\n",
      "episode_done:  True\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "episode_durations = []\n",
    "total_eps = 8\n",
    "TARGET_UPDATE = 5\n",
    "\n",
    "for i_ep in range(total_eps):\n",
    "    ### Reset the environment before the start of each episode\n",
    "    print('episode_no: ', i_ep)\n",
    "    env.reset()\n",
    "\n",
    "    ### Difference between the previous image and current image is provided as state\n",
    "    ### Not the output from the env.step-> which gives state values-> theta_1, theta_2, dtheta_1, dtheta_2\n",
    "    prev_im = get_image()\n",
    "    cur_im = get_image()\n",
    "    state = cur_im - prev_im\n",
    "\n",
    "    tt = 0\n",
    "    while True:\n",
    "        tt = tt + 1\n",
    "        ### Get action based on e-greedy algorithm\n",
    "        action = get_action(state)\n",
    "        ### Perform environment step to get the reward and done condition\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # The current image now becomes the prev image and also get the current image\n",
    "        prev_im = cur_im\n",
    "        #cur_im = get_image()\n",
    "        if not done:\n",
    "            next_state = get_image() - prev_im\n",
    "        else:\n",
    "            print('episode_done: ', done)\n",
    "            episode_durations.append(tt)\n",
    "            next_state = None\n",
    "\n",
    "        ### Store the transition array for current instant in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Next state now becomes current state\n",
    "        state = next_state\n",
    "\n",
    "        ### Train the neural network in this step by performing SGD based optimization \n",
    "        train_model()\n",
    "        \n",
    "        ### Finally if done, then break out of the loop after the training step. \n",
    "        ### This marks the end of the episode after which the state of the acrobot is again reset\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Update the target network\n",
    "    if i_ep % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
