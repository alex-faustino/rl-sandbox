{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-3-980047f4c312>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-980047f4c312>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    def my_policy(self):\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "class qLearning(object):\n",
    "    def __init__(self): \n",
    "        self.render_label = 'f'\n",
    "        self.gridnum = int(7) #size of gridworld\n",
    "        self.location_x = np.random.randint(int(1),self.gridnum-1)\n",
    "        self.location_y = np.random.randint(int(1),self.gridnum-1)\n",
    "        self.action = int(0) # no initial action until computed\n",
    "        self.previous_action = self.action\n",
    "        self.previous_x = self.location_x\n",
    "        self.previous_y = self.location_y\n",
    "        self.previous_previous_x = self.previous_x\n",
    "        self.previous_previous_y = self.previous_y\n",
    "        self.allowed_actions = np.array([1,2,3,4])[np.newaxis]\n",
    "        self.actionSet = np.matrix([1,2,3,4])\n",
    "        self.episode_length = int(100)\n",
    "        self.num_episodes = int(10000) # currently only running one episode, 10000\n",
    "        self.my_alpha = 0.1\n",
    "        self.my_gamma = 0.9\n",
    "        self.my_epsilon = 0.1\n",
    "        self.my_reward =np.array([ [-1,-1,-1,-1,-1,-1,-1],[-1, 0, 10, 0, 5, 0, -1],[-1, 0, 0, 0, 0, 0, -1],\\\n",
    "    [-1, 0, 0, 0, 0, 0, -1],[-1, 0, 0, 0, 0, 0, -1],[-1, 0, 0, 0, 0, 0, -1],[-1, -1, -1, -1, -1, -1, -1] ]) \n",
    "        self.my_reward = np.flipud(self.my_reward)\n",
    "        self.my_reward_model = np.zeros([self.gridnum,self.gridnum])#reward model updated based on observations\n",
    "        self.my_q_function = np.random.rand(self.gridnum,self.gridnum, int(4))# randomly initialized via reward model\n",
    "        self.my_q_function[0::6,:,:] = 0# to prevent biasing agent with favorable \"out of bound\" q functions\n",
    "        self.my_q_function[:,0::6,:] = 0# to prevent biasing agent with favorable \"out of bound\" q functions\n",
    "        self.my_reward_log = np.random.rand(1, self.episode_length*self.num_episodes) # used to store reward for each time step\n",
    "        self.my_episodic_cumulative_reward_log = np.random.rand(1,self.num_episodes)\n",
    "        self.update_q_label = 0\n",
    "        self.color_array = ['blue','orange']\n",
    "        self.episode_counter = 0\n",
    "        self.my_exploit_action_log = np.random.rand(self.gridnum,self.gridnum)\n",
    "        self.my_state_log = np.random.rand(2, self.episode_length*self.num_episodes)\n",
    "        pass\n",
    "    def my_policy(self):\n",
    "        if self.update_q_label > 1:\n",
    "            self.previous_action = self.action # still want this because the agent selected a poor action and should evaluate it\n",
    "        self.action = self.allowed_actions[0,\\\n",
    "        np.argmax(self.my_q_function[self.location_x,self.location_y,self.allowed_actions-1])]\n",
    "        self.my_exploit_action_log[self.location_y,self.location_x] = int(self.action)\n",
    "        if np.random.rand() <= self.my_epsilon:\n",
    "            self.action = self.allowed_actions[0,np.random.randint(0,self.allowed_actions.shape[1])]\n",
    "        return self.location_y,self.location_x,self.action\n",
    "    def update_reward_model(self):# reversed y and x for reward (not model) to accommodate human-readable reward\n",
    "        self.my_reward_model[self.location_y,self.location_x] = self.my_reward[self.location_y,self.location_x]\n",
    "        pass\n",
    "    def update_my_q_function(self):#,action,location_x,location_y):\n",
    "        if self.update_q_label > 1:# update_q_label ensures that we do not update the q function when location is reset\n",
    "            self.my_q_function[self.previous_previous_x,self.previous_previous_y,self.previous_action-1] +=\\\n",
    "            self.my_alpha*(self.my_reward_model[self.previous_previous_y,self.previous_previous_x]+\\\n",
    "            self.my_gamma*np.amax(self.my_q_function[self.previous_x,self.previous_y,:])-\\\n",
    "            self.my_q_function[self.previous_previous_x,self.previous_previous_y,self.previous_action-1])\n",
    "        pass\n",
    "world = gridworld()\n",
    "if world.render_label == 't':\n",
    "        fig, (ax) = world.render_init()\n",
    "k=0 # counter for episodic cumulative reward\n",
    "for i in range(1,world.episode_length * world.num_episodes - 1):\n",
    "    world.update_reward_model()\n",
    "    world.my_reward_log[0,i] = world.my_reward[world.location_y,world.location_x]\n",
    "    world.my_state_log[:,i] = np.array([world.location_x,world.location_y])[np.newaxis]\n",
    "    world.update_my_q_function()#update is for previous state, so put before state reversion\n",
    "    world.update_q_label += 1# default is to update the q function after the first iteration\n",
    "    if world.my_reward[world.location_y,world.location_x] < 0:\n",
    "        world.location_x = world.previous_x\n",
    "        world.location_y = world.previous_y\n",
    "    world.my_policy() # closest to pragmatic results here\n",
    "    world.step()# current state is now state AFTER action has been taken\n",
    "    if np.mod(i+1,world.episode_length) == 0:\n",
    "        world.my_episodic_cumulative_reward_log[0,k] = \\\n",
    "        np.sum(world.my_reward_log[0,(k*world.episode_length):(i+1)])# sums from k*episode_length to i\n",
    "        k += 1\n",
    "        world.reset()\n",
    "    progress_checker = np.floor(0.1*world.episode_length*world.num_episodes)\n",
    "    if np.mod(i+1,progress_checker) == 0:\n",
    "        sys.stdout.write(\"\\r\"+\"%s\" % int(10+np.floor(i/progress_checker)*10) + '%')#updates progress without excessive output\n",
    "    if world.render_label == 't':\n",
    "        world.render(fig,ax,i)\n",
    "sys.stdout.write(\"\\r\"+'done' + '\\n')#displays complete progress and prints results on new lines\n",
    "fig1, (ax1)=plt.subplots()\n",
    "ax1.plot(world.my_episodic_cumulative_reward_log[0,0:-1])\n",
    "plt.xlabel('episode number')\n",
    "plt.ylabel('total episodic reward')\n",
    "print('reward model' + '\\n' + str(np.flipud(world.my_reward_model[1:6,1:6])))\n",
    "print('Q function for up action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,0]))))\n",
    "print('Q function for down action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,1]))))\n",
    "print('Q function for left action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,2]))))\n",
    "print('Q function for right action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,3]))))\n",
    "print('total reward = ' + str(np.sum(world.my_reward_log[0,-world.episode_length:-1])))\n",
    "fig2, (ax2) = plt.subplots()\n",
    "ax2.plot(world.my_reward_log[0,-world.episode_length:-1])\n",
    "plt.xlabel('time step of episode')\n",
    "plt.ylabel('reward')\n",
    "fig3, (ax3) = plt.subplots()\n",
    "ax3.plot(np.transpose(world.my_state_log[0,-world.episode_length:-1]), label='x coordinate')\n",
    "ax3.plot(np.transpose(world.my_state_log[1,-world.episode_length:-1]), label='y coordinate')\n",
    "plt.xlabel('time step of episode')\n",
    "plt.ylabel('x and y coordinates')\n",
    "pylab.legend(loc='upper left')\n",
    "print('Exploit policy of agent, where: 1 is up, 2 is down, 3 is left and 4 is right')\n",
    "print(np.flipud(world.my_exploit_action_log[1:6,1:6]).astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
