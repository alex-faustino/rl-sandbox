{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation of Previous Code into Environment and Agent Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after policy current 3 1\n",
      "after policy current 3 0\n",
      "episode 0 time step 3\n",
      "current then previous 3 0 3 1\n",
      "current 3 1\n",
      "after policy current 3 -1\n",
      "episode 0 time step 4\n",
      "current then previous 3 -1 3 0\n",
      "current 3 0\n",
      "after policy current 3 -2\n",
      "after policy current 3 -1\n",
      "episode 0 time step 6\n",
      "current then previous 3 -1 3 -2\n",
      "current 3 -2\n",
      "after policy current 2 -1\n",
      "episode 0 time step 7\n",
      "current then previous 2 -1 3 -1\n",
      "current 3 -1\n",
      "after policy current 1 -1\n",
      "episode 0 time step 8\n",
      "current then previous 1 -1 2 -1\n",
      "current 2 -1\n",
      "after policy current 1 0\n",
      "episode 0 time step 9\n",
      "current then previous 1 0 1 -1\n",
      "current 1 -1\n",
      "after policy current 1 1\n",
      "after policy current 1 0\n",
      "episode 0 time step 11\n",
      "current then previous 1 0 1 1\n",
      "current 1 1\n",
      "after policy current 0 0\n",
      "episode 0 time step 12\n",
      "current then previous 0 0 1 0\n",
      "current 1 0\n",
      "after policy current -1 0\n",
      "episode 0 time step 13\n",
      "current then previous -1 0 0 0\n",
      "current 0 0\n",
      "after policy current -1 -1\n",
      "episode 0 time step 14\n",
      "current then previous -1 -1 -1 0\n",
      "current -1 0\n",
      "after policy current -1 0\n",
      "episode 0 time step 15\n",
      "current then previous -1 0 -1 -1\n",
      "current -1 -1\n",
      "after policy current -1 1\n",
      "episode 0 time step 16\n",
      "current then previous -1 1 -1 0\n",
      "current -1 0\n",
      "after policy current -1 2\n",
      "episode 0 time step 17\n",
      "current then previous -1 2 -1 1\n",
      "current -1 1\n",
      "after policy current -1 1\n",
      "episode 0 time step 18\n",
      "current then previous -1 1 -1 2\n",
      "current -1 2\n",
      "after policy current -1 2\n",
      "episode 0 time step 19\n",
      "current then previous -1 2 -1 1\n",
      "current -1 1\n",
      "after policy current -1 3\n",
      "episode 0 time step 20\n",
      "current then previous -1 3 -1 2\n",
      "current -1 2\n",
      "after policy current -1 4\n",
      "episode 0 time step 21\n",
      "current then previous -1 4 -1 3\n",
      "current -1 3\n",
      "after policy current -1 3\n",
      "episode 0 time step 22\n",
      "current then previous -1 3 -1 4\n",
      "current -1 4\n",
      "after policy current -1 4\n",
      "episode 0 time step 23\n",
      "current then previous -1 4 -1 3\n",
      "current -1 3\n",
      "after policy current -1 5\n",
      "episode 0 time step 24\n",
      "current then previous -1 5 -1 4\n",
      "current -1 4\n",
      "after policy current -1 6\n",
      "episode 0 time step 25\n",
      "current then previous -1 6 -1 5\n",
      "current -1 5\n",
      "after policy current -1 5\n",
      "episode 0 time step 26\n",
      "current then previous -1 5 -1 6\n",
      "current -1 6\n",
      "after policy current -1 6\n",
      "episode 0 time step 27\n",
      "current then previous -1 6 -1 5\n",
      "current -1 5\n",
      "after policy current -1 5\n",
      "episode 0 time step 28\n",
      "current then previous -1 5 -1 6\n",
      "current -1 6\n",
      "after policy current -1 6\n",
      "episode 0 time step 29\n",
      "current then previous -1 6 -1 5\n",
      "current -1 5\n",
      "after policy current -1 5\n",
      "episode 0 time step 30\n",
      "current then previous -1 5 -1 6\n",
      "current -1 6\n",
      "after policy current -1 6\n",
      "episode 0 time step 31\n",
      "current then previous -1 6 -1 5\n",
      "current -1 5\n",
      "after policy current -1 5\n",
      "episode 0 time step 32\n",
      "current then previous -1 5 -1 6\n",
      "current -1 6\n",
      "after policy current 0 5\n",
      "episode 0 time step 33\n",
      "current then previous 0 5 -1 5\n",
      "current -1 5\n",
      "after policy current 0 4\n",
      "episode 0 time step 34\n",
      "current then previous 0 4 0 5\n",
      "current 0 5\n",
      "after policy current -1 4\n",
      "episode 0 time step 35\n",
      "current then previous -1 4 0 4\n",
      "current 0 4\n",
      "after policy current -1 5\n",
      "episode 0 time step 36\n",
      "current then previous -1 5 -1 4\n",
      "current -1 4\n",
      "after policy current -2 5\n",
      "after policy current -2 4\n",
      "after policy current -1 4\n",
      "episode 0 time step 39\n",
      "current then previous -1 4 -2 4\n",
      "current -2 4\n",
      "after policy current -2 4\n",
      "after policy current -3 4\n",
      "after policy current -4 4\n",
      "after policy current -4 3\n",
      "after policy current -3 3\n",
      "after policy current -3 2\n",
      "after policy current -4 2\n",
      "after policy current -4 1\n",
      "after policy current -4 2\n",
      "after policy current -4 1\n",
      "after policy current -4 2\n",
      "after policy current -4 1\n",
      "after policy current -4 2\n",
      "after policy current -4 1\n",
      "after policy current -4 2\n",
      "after policy current -4 1\n",
      "after policy current -4 2\n",
      "after policy current -4 1\n",
      "after policy current -4 0\n",
      "episode 0 time step 58\n",
      "current then previous -4 0 -4 1\n",
      "current -4 1\n",
      "after policy current -4 -1\n",
      "episode 0 time step 59\n",
      "current then previous -4 -1 -4 0\n",
      "current -4 0\n",
      "after policy current -4 -2\n",
      "after policy current -4 -1\n",
      "episode 0 time step 61\n",
      "current then previous -4 -1 -4 -2\n",
      "current -4 -2\n",
      "after policy current -4 0\n",
      "episode 0 time step 62\n",
      "current then previous -4 0 -4 -1\n",
      "current -4 -1\n",
      "after policy current -4 -1\n",
      "episode 0 time step 63\n",
      "current then previous -4 -1 -4 0\n",
      "current -4 0\n",
      "after policy current -4 -2\n",
      "after policy current -4 -1\n",
      "episode 0 time step 65\n",
      "current then previous -4 -1 -4 -2\n",
      "current -4 -2\n",
      "after policy current -5 -1\n",
      "episode 0 time step 66\n",
      "current then previous -5 -1 -4 -1\n",
      "current -4 -1\n",
      "after policy current -6 -1\n",
      "episode 0 time step 67\n",
      "current then previous -6 -1 -5 -1\n",
      "current -5 -1\n",
      "after policy current -5 -1\n",
      "episode 0 time step 68\n",
      "current then previous -5 -1 -6 -1\n",
      "current -6 -1\n",
      "after policy current -5 0\n",
      "episode 0 time step 69\n",
      "current then previous -5 0 -5 -1\n",
      "current -5 -1\n",
      "after policy current -5 -1\n",
      "episode 0 time step 70\n",
      "current then previous -5 -1 -5 0\n",
      "current -5 0\n",
      "after policy current -5 0\n",
      "episode 0 time step 71\n",
      "current then previous -5 0 -5 -1\n",
      "current -5 -1\n",
      "after policy current -5 1\n",
      "after policy current -4 1\n",
      "after policy current -3 1\n",
      "after policy current -3 0\n",
      "episode 0 time step 75\n",
      "current then previous -3 0 -3 1\n",
      "current -3 1\n",
      "after policy current -3 1\n",
      "after policy current -3 2\n",
      "after policy current -3 1\n",
      "after policy current -3 0\n",
      "episode 0 time step 79\n",
      "current then previous -3 0 -3 1\n",
      "current -3 1\n",
      "after policy current -3 1\n",
      "after policy current -3 2\n",
      "after policy current -3 1\n",
      "after policy current -3 0\n",
      "episode 0 time step 83\n",
      "current then previous -3 0 -3 1\n",
      "current -3 1\n",
      "after policy current -3 -1\n",
      "episode 0 time step 84\n",
      "current then previous -3 -1 -3 0\n",
      "current -3 0\n",
      "after policy current -3 -2\n",
      "after policy current 4 3\n",
      "after policy current 4 2\n",
      "after policy current 5 2\n",
      "after policy current 5 1\n",
      "after policy current 5 0\n",
      "episode 0 time step 90\n",
      "current then previous 5 0 5 1\n",
      "current 5 1\n",
      "after policy current 6 0\n",
      "episode 0 time step 91\n",
      "current then previous 6 0 5 0\n",
      "current 5 0\n",
      "after policy current 7 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 1 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-771aa28b758c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#world.work()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_Learning_Agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/floppsy/Desktop/598rl/homework/hw3a/hw3a_allana2_catching_up/Q_Learning_Agent.py\u001b[0m in \u001b[0;36mwork\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;31m# counter for episodic cumulative reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_reward_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_reward_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_state_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floppsy/Desktop/598rl/homework/hw3a/hw3a_allana2_catching_up/Q_Learning_Agent.py\u001b[0m in \u001b[0;36mupdate_reward_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;31m# self.location_y,self.location_x,self.action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_reward_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# reversed y and x for reward (not model) to accommodate human-readable reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_reward_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_x\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_my_q_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#,action,location_x,location_y):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for axis 1 with size 7"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "#import importlib\n",
    "\n",
    "import Gridworld_Env, Q_Learning_Agent #importing seems to run the files?\n",
    "#grid_world = importlib.reload(grid_world)\n",
    "#sarsa = importlib.reload(sarsa)\n",
    "\n",
    "world = Gridworld_Env.gridworld()\n",
    "#world.work()\n",
    "agent = Q_Learning_Agent.qLearning(world)\n",
    "agent.work()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
