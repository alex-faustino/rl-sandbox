{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation of Previous Code into Environment and Agent Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "import Gridworld_Env, Q_Learning_Agent\n",
    "\n",
    "world = Gridworld_Env.gridworld()\n",
    "agent = Q_Learning_Agent.qLearning(world,'')#'render' with quotes to show environment\n",
    "agent.work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Using PyTorch\n",
    "The neural network model uses array-style inputs and outputs, so I have to change my Q function from it's original form in Q_Learning_Agent to an arrayed form in Q_Learning_Agent_Arrayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "import Gridworld_Env, Q_Learning_Agent_Arrayed\n",
    "\n",
    "world = Gridworld_Env.gridworld()\n",
    "agent = Q_Learning_Agent_Arrayed.qLearning(world,'')#'render' with quotes to show environment\n",
    "agent.work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Used to PyTorch\n",
    "The below code seems to work, now making it into a class file. Code from https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "However, upon biasing the output deterministically, we see that the learning algorithm is not stable in its performance. However, the perfomance is stable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 10, 1, 100, 4\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)*10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)#prediction step is called forward pass\n",
    "    \n",
    "#    print(y_pred)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)#loss calculation for feedback\n",
    "    print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()#gradient of loss step is called backward pass\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering models between off-policy network and target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred\n",
    "model2 = model\n",
    "y_pred2 = model2(x)\n",
    "print(y_pred)\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Deep Q-Learning Agent in Gridworld Environment\n",
    "## Steps for implementation\n",
    "* Weights initialized \n",
    "### Looping\n",
    "* NN \"forward pass\" (prediction of Q function) is sent to Agent\n",
    "* Agent makes decision and observes reward. The reward is sent back to NN along with the currently used Q function\n",
    "* Send Current Q function estimate for the agent's exploiting policy action (regardless if agent performs action) and newly observed reward to NN class file\n",
    "* Agent reperforms \"forward pass\" (prediction of Q function) to facilitate the calculation of loss with respect to the received Q function estimate for agent's exploit action and the observed reward\n",
    "* Repeat but without weight initialization\n",
    "\n",
    "## Progress\n",
    "### Done\n",
    "* Weights already randomly initialized using above code\n",
    "### Need to Do\n",
    "* Forward pass sent to agent\n",
    " * Need to include NN in agent's __init__ to enable receiving forward pass output for actions related to agent's state\n",
    " * Need to include agent in NN's __init__ to receive state, reward and to receive Q function estimate (y_pred) from agent\n",
    "#### Progress Notes\n",
    "Example code, upon which my implementation is based, randomly initizlizes input and output to the network, but not necessarily the weights? From reviewing the tutorial, it seems that the weights are randomly initialized as part of the establishment of a model network. I'll operate on that assumption for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "import Gridworld_Env_General, Q_Learning_Agent_nn\n",
    "import reluNetworkClass, reluNetworkClass2\n",
    "\n",
    "world = Gridworld_Env_General.gridworld()\n",
    "my_nn2 = reluNetworkClass2.qLearningNetwork(world)\n",
    "my_nn = reluNetworkClass.qLearningNetwork(world,my_nn2)\n",
    "agent = Q_Learning_Agent_nn.qLearning(world,my_nn,my_nn2,'')#'render' with quotes to show environment\n",
    "agent.work()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
