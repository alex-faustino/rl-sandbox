{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation of Previous Code into Environment and Agent Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "import Gridworld_Env, Q_Learning_Agent\n",
    "\n",
    "world = Gridworld_Env.gridworld()\n",
    "agent = Q_Learning_Agent.qLearning(world,'')#'render' with quotes to show environment\n",
    "agent.work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Using PyTorch\n",
    "The neural network model uses array-style inputs and outputs, so I have to change my Q function from it's original form in Q_Learning_Agent to an arrayed form in Q_Learning_Agent_Arrayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "import Gridworld_Env, Q_Learning_Agent_Arrayed\n",
    "\n",
    "world = Gridworld_Env.gridworld()\n",
    "agent = Q_Learning_Agent_Arrayed.qLearning(world,'')#'render' with quotes to show environment\n",
    "agent.work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Used to PyTorch\n",
    "The below code seems to work, now making it into a class file. Code from https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "However, upon biasing the output deterministically, we see that the learning algorithm is not stable in its performance. However, the perfomance is stable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 10, 1, 100, 4\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "#xx = torch.randn(D_in,1)\n",
    "y = torch.randn(N, D_out)*10\n",
    "#y[0] = torch.tensor([0,10,0,5])\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, H+100, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H+100, H, bias=True),\n",
    "#    torch.nn.ReLU(),\n",
    "#    torch.nn.Linear(H+50, H, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)#prediction step is called forward pass\n",
    "    \n",
    "#    print(y_pred)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)#loss calculation for feedback\n",
    "    print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()#gradient of loss step is called backward pass\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making it a class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "class test(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "    \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "        self.N, self.D_in, self.H, self.D_out = 10, 1, 100, 4\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "        self.x = torch.randn(self.N, self.D_in)\n",
    "#xx = torch.randn(D_in,1)\n",
    "        self.y = torch.randn(self.N, self.D_out)*10\n",
    "#y[0] = torch.tensor([0,10,0,5])\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.D_in, self.H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.H, self.H+100, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.H+100, self.H, bias=True),\n",
    "#    torch.nn.ReLU(),\n",
    "#    torch.nn.Linear(H+50, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.H, self.D_out),\n",
    ")\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        self.learning_rate = 1e-3\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        pass\n",
    "    def predict(self,x):\n",
    "        self.y_pred = self.model(x)\n",
    "        return self.y_pred\n",
    "    def update(self,t):\n",
    "            self.loss = self.loss_fn(self.y_pred, self.y)#loss calculation for feedback\n",
    "            \n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            self.loss.backward()#gradient of loss step is called backward pass\n",
    "            self.optimizer.step()\n",
    "            return print(t, self.loss.item())\n",
    "    def work(self):\n",
    "        for t in range(2000):\n",
    "            #y_pred = model(x)#prediction step is called forward pass\n",
    "            self.y_pred = self.predict(self.x)\n",
    "            self.update(t)\n",
    "            print(self.y_pred)\n",
    "            print(self.y)\n",
    "            pass\n",
    "#    print(y_pred)\n",
    "    \n",
    "testy = test()\n",
    "testy.work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering models between off-policy network and target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred\n",
    "model2 = model\n",
    "y_pred2 = model2(x)\n",
    "print(y_pred)\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Deep Q-Learning Agent in Gridworld Environment\n",
    "## Steps for implementation\n",
    "* Weights initialized \n",
    "### Looping\n",
    "* NN \"forward pass\" (prediction of Q function) is sent to Agent\n",
    "* Agent makes decision and observes reward. The reward is sent back to NN along with the currently used Q function\n",
    "* Send Current Q function estimate for the agent's exploiting policy action (regardless if agent performs action) and newly observed reward to NN class file\n",
    "* Agent reperforms \"forward pass\" (prediction of Q function) to facilitate the calculation of loss with respect to the received Q function estimate for agent's exploit action and the observed reward\n",
    "* Repeat but without weight initialization\n",
    "\n",
    "## Progress\n",
    "### Done\n",
    "* Weights already randomly initialized using above code\n",
    "### Need to Do\n",
    "* Forward pass sent to agent\n",
    " * Need to include NN in agent's __init__ to enable receiving forward pass output for actions related to agent's state\n",
    " * Need to include agent in NN's __init__ to receive state, reward and to receive Q function estimate (y_pred) from agent\n",
    "#### Progress Notes\n",
    "Example code, upon which my implementation is based, randomly initizlizes input and output to the network, but not necessarily the weights? From reviewing the tutorial, it seems that the weights are randomly initialized as part of the establishment of a model network. I'll operate on that assumption for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0213, -0.0443,  0.0751,  0.0023],\n",
      "        [-0.0192, -0.0260,  0.0683,  0.0025],\n",
      "        [-0.0146, -0.0666,  0.0872, -0.0055],\n",
      "        [-0.0213, -0.0446,  0.0752,  0.0020],\n",
      "        [-0.0198, -0.0475,  0.0781, -0.0018],\n",
      "        [-0.0216, -0.0451,  0.0756,  0.0012],\n",
      "        [-0.0236,  0.0011,  0.0495,  0.0231],\n",
      "        [-0.0223, -0.0228,  0.0654, -0.0053],\n",
      "        [-0.0376,  0.0199,  0.0520,  0.0470],\n",
      "        [-0.0291,  0.0100,  0.0500,  0.0372],\n",
      "        [-0.0257, -0.0198,  0.0653, -0.0075],\n",
      "        [-0.0248, -0.0167,  0.0633, -0.0088],\n",
      "        [-0.0232, -0.0138,  0.0610, -0.0078],\n",
      "        [-0.0100, -0.0732,  0.0931, -0.0070],\n",
      "        [-0.0237, -0.0149,  0.0621, -0.0085],\n",
      "        [-0.0191, -0.0483,  0.0788, -0.0021],\n",
      "        [-0.0195, -0.0255,  0.0674,  0.0009],\n",
      "        [-0.0197, -0.0254,  0.0671,  0.0003],\n",
      "        [-0.0242, -0.0156,  0.0627, -0.0087],\n",
      "        [-0.0180, -0.0503,  0.0800, -0.0027],\n",
      "        [-0.0217, -0.0039,  0.0529,  0.0075]], grad_fn=<ThAddmmBackward>)\n",
      "predict\n",
      "tensor([-0.0189, -0.0277,  0.0701,  0.0046], grad_fn=<ThAddBackward>)\n",
      "predict\n",
      "tensor([-0.0189, -0.0277,  0.0701,  0.0046], grad_fn=<ThAddBackward>)\n",
      "predict\n",
      "tensor([-0.0189, -0.0277,  0.0701,  0.0046], grad_fn=<ThAddBackward>)\n",
      "predict\n",
      "tensor([-0.0189, -0.0277,  0.0701,  0.0046], grad_fn=<ThAddBackward>)\n",
      "predict\n",
      "tensor([-0.0189, -0.0277,  0.0701,  0.0046], grad_fn=<ThAddBackward>)\n",
      "predict\n",
      "tensor([-0.0189, -0.0277,  0.0701,  0.0046], grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-104e80f296af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmy_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreluNetworkClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqLearningNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_nn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_Learning_Agent_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_nn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#replace '' with 'render' to show environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/floppsy/Desktop/598rl/homework/hw3a/hw3a_allana2_catching_up/Q_Learning_Agent_nn.py\u001b[0m in \u001b[0;36mwork\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m## train main neural network ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m              \u001b[0mtemporary_nn_main_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporary_nn_main_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_gamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m## train target neural network (logic in class file to only update when appropriate) ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floppsy/Desktop/598rl/homework/hw3a/hw3a_allana2_catching_up/reluNetworkClass.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, state, reward, discount, previous_state, action)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#gradient of loss step is called backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floppsy/anaconda3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floppsy/anaconda3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "import Gridworld_Env_General, Q_Learning_Agent_nn\n",
    "import reluNetworkClass, reluNetworkClass2\n",
    "\n",
    "world = Gridworld_Env_General.gridworld()\n",
    "my_nn2 = reluNetworkClass2.qLearningNetwork(world)\n",
    "my_nn = reluNetworkClass.qLearningNetwork(world,my_nn2)\n",
    "agent = Q_Learning_Agent_nn.qLearning(world,my_nn,my_nn2,'')#replace '' with 'render' to show environment\n",
    "agent.work()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
