{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "class gridworld(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self): \n",
    "    self.render_label = 'f'\n",
    "    self.gridnum = int(7) #size of gridworld\n",
    "    self.location_x = np.random.randint(int(1),self.gridnum-1)\n",
    "    self.location_y = np.random.randint(int(1),self.gridnum-1)\n",
    "    self.action = int(0) # no initial action until computed\n",
    "    self.previous_action = self.action\n",
    "    self.previous_x = self.location_x\n",
    "    self.previous_y = self.location_y\n",
    "    self.previous_previous_x = self.previous_x\n",
    "    self.previous_previous_y = self.previous_y\n",
    "    self.allowed_actions = np.array([1,2,3,4])[np.newaxis]\n",
    "    self.actionSet = np.matrix([1,2,3,4])\n",
    "    self.episode_length = int(100)\n",
    "    self.num_episodes = int(10000) # currently only running one episode, 10000\n",
    "    self.my_alpha = 0.1\n",
    "    self.my_gamma = 0.9\n",
    "    self.my_epsilon = 0.1\n",
    "    self.my_reward =np.array([ [-1,-1,-1,-1,-1,-1,-1],[-1, 0, 10, 0, 5, 0, -1],[-1, 0, 0, 0, 0, 0, -1],\\\n",
    "    [-1, 0, 0, 0, 0, 0, -1],[-1, 0, 0, 0, 0, 0, -1],[-1, 0, 0, 0, 0, 0, -1],[-1, -1, -1, -1, -1, -1, -1] ]) \n",
    "    self.my_reward = np.flipud(self.my_reward)\n",
    "    self.my_reward_model = np.zeros([self.gridnum,self.gridnum])#reward model updated based on observations\n",
    "    self.my_q_function = np.random.rand(self.gridnum,self.gridnum, int(4))# randomly initialized via reward model\n",
    "    self.my_q_function[0::6,:,:] = 0# to prevent biasing agent with favorable \"out of bound\" q functions\n",
    "    self.my_q_function[:,0::6,:] = 0# to prevent biasing agent with favorable \"out of bound\" q functions\n",
    "    self.my_reward_log = np.random.rand(1, self.episode_length*self.num_episodes) # used to store reward for each time step\n",
    "    self.my_episodic_cumulative_reward_log = np.random.rand(1,self.num_episodes)\n",
    "    self.update_q_label = 0\n",
    "    self.color_array = ['blue','orange']\n",
    "    self.episode_counter = 0\n",
    "    self.my_exploit_action_log = np.random.rand(self.gridnum,self.gridnum)\n",
    "    self.my_state_log = np.random.rand(2, self.episode_length*self.num_episodes)\n",
    "    pass\n",
    "  def render(self,fig,ax,time_index): #mode='human', close=False <- no idea what this is for\n",
    "    ax = fig.gca()    \n",
    "    ax.clear()\n",
    "    ax.grid(which='major', axis='both', linestyle='-')\n",
    "    circle2 = plt.Circle((world.location_x+0.5, world.location_y+0.5), 0.5, color=self.color_array[np.mod(self.episode_counter,2)])#rand initialization\n",
    "    ax.add_artist(circle2)\n",
    "    fig.canvas.draw()\n",
    "    pass\n",
    "  def render_init(self):\n",
    "    fig, (ax)=plt.subplots()\n",
    "    intervals = float(1/world.gridnum)# dimension of grid affects size\n",
    "    loc = plticker.MultipleLocator(base=intervals)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.set_xlim(0, world.gridnum)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    ax.set_ylim(0, world.gridnum)\n",
    "    return fig, ax\n",
    "  def reset(self):\n",
    "    self.location_x = np.random.randint(int(1),self.gridnum-1)\n",
    "    self.location_y = np.random.randint(int(1),self.gridnum-1)\n",
    "    self.previous_x = self.location_x\n",
    "    self.previous_y = self.location_y\n",
    "    self.previous_previous_x = self.previous_x\n",
    "    self.previous_previous_y = self.previous_y\n",
    "    self.update_q_label = 0\n",
    "    self.episode_counter += 1\n",
    "    pass\n",
    "  def step(self):\n",
    "    self.previous_previous_x = self.previous_x\n",
    "    self.previous_previous_y = self.previous_y\n",
    "    self.previous_x = self.location_x# only accurate for second step in episode\n",
    "    self.previous_y = self.location_y# only accurate for second step in episode\n",
    "    desired_action = self.action\n",
    "    if np.random.rand() <= 0.1: # this part of the method is to enforce a 10% chance of a random transition\n",
    "        self.action = self.allowed_actions[0,np.random.randint(1,self.allowed_actions.shape[1])]\n",
    "    if self.my_reward[self.location_y,self.location_x] > 0:\n",
    "        self.location_x = 3+1\n",
    "        self.location_y = 2+1\n",
    "        if self.my_reward[self.previous_y,self.previous_x] > 5:\n",
    "            self.location_x = 1+1\n",
    "            self.location_y = 0+1\n",
    "    elif self.action == 1: # this part of the method is to select the desired deterministic action\n",
    "        self.location_y += 1\n",
    "    elif self.action == 2:\n",
    "        self.location_y += -1\n",
    "    elif self.action == 3:\n",
    "        self.location_x += -1\n",
    "    elif self.action == 4:\n",
    "        self.location_x += 1\n",
    "    self.action = desired_action# reports selected action, but has the agent move according to transition probability\n",
    "    pass #return [(self.location_x, self.location_y)]\n",
    "world = gridworld()\n",
    "if world.render_label == 't':\n",
    "    fig, (ax) = world.render_init()\n",
    "k=0 # counter for episodic cumulative reward\n",
    "for i in range(1,world.episode_length * world.num_episodes - 1):\n",
    "    world.update_reward_model()\n",
    "    world.my_reward_log[0,i] = world.my_reward[world.location_y,world.location_x]\n",
    "    world.my_state_log[:,i] = np.array([world.location_x,world.location_y])[np.newaxis]\n",
    "    world.update_my_q_function()#update is for previous state, so put before state reversion\n",
    "    world.update_q_label += 1# default is to update the q function after the first iteration\n",
    "    if world.my_reward[world.location_y,world.location_x] < 0:\n",
    "        world.location_x = world.previous_x\n",
    "        world.location_y = world.previous_y\n",
    "    world.my_policy() # closest to pragmatic results here\n",
    "    world.step()# current state is now state AFTER action has been taken\n",
    "    if np.mod(i+1,world.episode_length) == 0:\n",
    "        world.my_episodic_cumulative_reward_log[0,k] = \\\n",
    "        np.sum(world.my_reward_log[0,(k*world.episode_length):(i+1)])# sums from k*episode_length to i\n",
    "        k += 1\n",
    "        world.reset()\n",
    "    progress_checker = np.floor(0.1*world.episode_length*world.num_episodes)\n",
    "    if np.mod(i+1,progress_checker) == 0:\n",
    "        sys.stdout.write(\"\\r\"+\"%s\" % int(10+np.floor(i/progress_checker)*10) + '%')#updates progress without excessive output\n",
    "    if world.render_label == 't':\n",
    "        world.render(fig,ax,i)\n",
    "sys.stdout.write(\"\\r\"+'done' + '\\n')#displays complete progress and prints results on new lines\n",
    "fig1, (ax1)=plt.subplots()\n",
    "ax1.plot(world.my_episodic_cumulative_reward_log[0,0:-1])\n",
    "plt.xlabel('episode number')\n",
    "plt.ylabel('total episodic reward')\n",
    "print('reward model' + '\\n' + str(np.flipud(world.my_reward_model[1:6,1:6])))\n",
    "print('Q function for up action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,0]))))\n",
    "print('Q function for down action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,1]))))\n",
    "print('Q function for left action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,2]))))\n",
    "print('Q function for right action' + '\\n' + str(np.flipud(np.transpose(world.my_q_function[1:6,1:6,3]))))\n",
    "print('total reward = ' + str(np.sum(world.my_reward_log[0,-world.episode_length:-1])))\n",
    "fig2, (ax2) = plt.subplots()\n",
    "ax2.plot(world.my_reward_log[0,-world.episode_length:-1])\n",
    "plt.xlabel('time step of episode')\n",
    "plt.ylabel('reward')\n",
    "fig3, (ax3) = plt.subplots()\n",
    "ax3.plot(np.transpose(world.my_state_log[0,-world.episode_length:-1]), label='x coordinate')\n",
    "ax3.plot(np.transpose(world.my_state_log[1,-world.episode_length:-1]), label='y coordinate')\n",
    "plt.xlabel('time step of episode')\n",
    "plt.ylabel('x and y coordinates')\n",
    "pylab.legend(loc='upper left')\n",
    "print('Exploit policy of agent, where: 1 is up, 2 is down, 3 is left and 4 is right')\n",
    "print(np.flipud(world.my_exploit_action_log[1:6,1:6]).astype(int))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
