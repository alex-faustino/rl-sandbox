{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 part A by Denis Osipychev\n",
    "### Testing my DQN on CartPole\n",
    "##### import gym and local environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the env and test it with a fixed action\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "n_episodes = 5000\n",
    "episode_lenght = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.001\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "epsilon = 0.7 # it will vary during the training\n",
    "n_neurons = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn structure for Q module\n",
    "class Qnetwork():\n",
    "    def __init__(self, input_size, layer_size, output_size):\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None,input_size],dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.input,layer_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden,output_size,biases_initializer=None)\n",
    "        \n",
    "        self.new_q = tf.placeholder(shape=[None,],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None,output_size],dtype=tf.float32)\n",
    "        self.Q_action = tf.reduce_sum(tf.multiply(self.output,self.action_holder), reduction_indices=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.new_q - self.Q_action))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=alpha)\n",
    "        self.update_model = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy policy\n",
    "def greedy_policy(Q):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        best_action = np.random.randint(env.action_space.n)\n",
    "    else:\n",
    "        best_action = np.argmax(Q)\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, arg):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = arg\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            batch_size = len(self.memory)\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the weights and network\n",
    "tf.reset_default_graph() # Clear the Tensorflow graph.\n",
    "myAgent = Qnetwork(env.observation_space.shape[0], n_neurons, env.action_space.n)\n",
    "merged = tf.summary.merge_all()\n",
    "buffer = ReplayMemory(buffer_size)\n",
    "epsilon_reduction = epsilon / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch the session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Initialized Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "i = 0\n",
    "\n",
    "# run the training\n",
    "while i < n_episodes:\n",
    "    \n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    i += 1\n",
    "    j = 0\n",
    "    d = False\n",
    "    \n",
    "    #run the simulation\n",
    "    while j < episode_lenght:\n",
    "        \n",
    "        j += 1\n",
    "        \n",
    "        q = sess.run(myAgent.output,feed_dict={myAgent.input:[s]})\n",
    "        action = greedy_policy(q)\n",
    "        \n",
    "        # do the step\n",
    "        s_new,r,d,_ = env.step(action)\n",
    "        \n",
    "        action_one_hot = np.zeros(env.action_space.n)\n",
    "        action_one_hot[action] = 1\n",
    "        \n",
    "        buffer.push([s,action_one_hot,r,d,s_new])\n",
    "        \n",
    "        total_reward += r\n",
    "        s = s_new\n",
    "        \n",
    "        if buffer.size() > batch_size:\n",
    "            st, at, rt, dt, snewt = zip(*buffer.sample(batch_size))\n",
    "            qt = sess.run(myAgent.output,feed_dict={myAgent.input:snewt})\n",
    "            yt = []\n",
    "            for k in range(batch_size):\n",
    "                if dt[k] == True:\n",
    "                    yt.append(rt[k])\n",
    "                else:\n",
    "                    yt.append(rt[k] + gamma*np.max(qt[k]))\n",
    "                \n",
    "            _ = sess.run(myAgent.update_model,feed_dict={myAgent.input:st,\n",
    "                                                         myAgent.action_holder:at,\n",
    "                                                         myAgent.new_q:yt})\n",
    "            \n",
    "        # update q\n",
    "        if d == True:\n",
    "            break\n",
    "            \n",
    "    # decay exploration\n",
    "    epsilon -= epsilon_reduction\n",
    "    epsilon = max(0.01, epsilon)\n",
    "    history.append([i, total_reward, epsilon, alpha, j])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(' Epoch:',i,',Average R:',np.mean(np.asarray(history)[-100:,1]),',Epsilon:', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the history\n",
    "h = np.asarray(history)\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(h[:,0], h[:,1], '-')\n",
    "plt.ylabel('reward')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(h[:,0], h[:,2], '-')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('epsilon')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(h[:,0], h[:,3], '-')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('alpha')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(h[:,0], h[:,4], '-')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('length ep')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = tf.train.get_checkpoint_state('./model')\n",
    "\n",
    "# if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "#     saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#     print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "# else:\n",
    "#     print(\"No policy for evaluation\")\n",
    "\n",
    "# evaluate the policy\n",
    "s = env.reset()\n",
    "total_reward = 0\n",
    "j = 0\n",
    "\n",
    "while j < episode_lenght:\n",
    "    j += 1\n",
    "    q = sess.run(myAgent.output,feed_dict={myAgent.input:[s]})\n",
    "    action = np.argmax(q)\n",
    "    s_new,r,d,_ = env.step(action)\n",
    "    total_reward += r\n",
    "    s = s_new\n",
    "    env.render()\n",
    "    print(\"step:\",j,\",r:\",r)\n",
    "print(\"total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 part B by Denis Osipychev\n",
    "### Training my DQN on Acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.05531235,  2.13833491, -1.29839659,  1.83169234])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import myenv\n",
    "#dir(myenv)\n",
    "\n",
    "# initialize the env\n",
    "env = myenv.AcrobotEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "n_episodes = 10000\n",
    "episode_lenght = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.001\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "epsilon = 0.7 # it will vary during the training\n",
    "n_neurons = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the weights and network\n",
    "tf.reset_default_graph() # Clear the Tensorflow graph.\n",
    "myAgent = Qnetwork(env.observation_space.shape[0], n_neurons, env.action_space.n)\n",
    "merged = tf.summary.merge_all()\n",
    "buffer = ReplayMemory(buffer_size)\n",
    "epsilon_reduction = epsilon / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/policy.ckpt\n",
      "Load Model :  ./model/policy.ckpt\n"
     ]
    }
   ],
   "source": [
    "# launch the session\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialize, save or restore the network \n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('./model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 100 ,Average R: 0.32 ,Epsilon: 0.6929999999999985\n",
      " Epoch: 200 ,Average R: 0.15 ,Epsilon: 0.6859999999999971\n",
      " Epoch: 300 ,Average R: 0.36 ,Epsilon: 0.6789999999999956\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "i = 0\n",
    "\n",
    "# run the training\n",
    "while i < n_episodes:\n",
    "    \n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    i += 1\n",
    "    j = 0\n",
    "    d = False\n",
    "    \n",
    "    #run the simulation\n",
    "    while j < episode_lenght:\n",
    "        \n",
    "        j += 1\n",
    "        \n",
    "        q = sess.run(myAgent.output,feed_dict={myAgent.input:[s]})\n",
    "        action = greedy_policy(q)\n",
    "        \n",
    "        # do the step\n",
    "        s_new,r,d,_ = env.step(action)\n",
    "        \n",
    "        action_one_hot = np.zeros(env.action_space.n)\n",
    "        action_one_hot[action] = 1\n",
    "        \n",
    "        buffer.push([s,action_one_hot,r,d,s_new])\n",
    "        \n",
    "#         q_next = sess.run(myAgent.output,feed_dict={myAgent.input:[s_new]})\n",
    "#         targetq = q.flatten()\n",
    "#         targetq[action] = r + gamma * np.max(q_next)\n",
    "        \n",
    "#         s_batch.append(s)\n",
    "#         q_batch.append(q_next)\n",
    "        \n",
    "        total_reward += r\n",
    "        s = s_new\n",
    "        \n",
    "        if buffer.size() > batch_size:\n",
    "            st, at, rt, dt, snewt = zip(*buffer.sample(batch_size))\n",
    "            qt = sess.run(myAgent.output,feed_dict={myAgent.input:snewt})\n",
    "            yt = []\n",
    "            for k in range(batch_size):\n",
    "                if dt[k] == True:\n",
    "                    yt.append(rt[k])\n",
    "                else:\n",
    "                    yt.append(rt[k] + gamma*np.max(qt[k]))\n",
    "                \n",
    "            _ = sess.run(myAgent.update_model,feed_dict={myAgent.input:st,\n",
    "                                                         myAgent.action_holder:at,\n",
    "                                                         myAgent.new_q:yt})\n",
    "            \n",
    "        # update q\n",
    "    #if d == True:\n",
    "            \n",
    "            \n",
    "    # decay exploration\n",
    "    epsilon -= epsilon_reduction\n",
    "    epsilon = max(0.01, epsilon)\n",
    "    history.append([i, total_reward, epsilon, alpha, j])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        # save the model\n",
    "        print(' Epoch:',i,',Average R:',np.mean(np.asarray(history)[-100:,1]),',Epsilon:', epsilon)\n",
    "        saver.save(sess, './model/policy.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the history\n",
    "h = np.asarray(history)\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(h[:,0], h[:,1], '-')\n",
    "plt.ylabel('reward')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(h[:,0], h[:,2], '-')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('epsilon')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(h[:,0], h[:,3], '-')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('alpha')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(h[:,0], h[:,4], '-')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('length ep')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = tf.train.get_checkpoint_state('./model')\n",
    "\n",
    "# if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "#     saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#     print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "# else:\n",
    "#     print(\"No policy for evaluation\")\n",
    "\n",
    "# evaluate the policy\n",
    "s = env.reset()\n",
    "total_reward = 0\n",
    "j = 0\n",
    "\n",
    "while j < episode_lenght:\n",
    "    j += 1\n",
    "    q = sess.run(myAgent.output,feed_dict={myAgent.input:[s]})\n",
    "    action = np.argmax(q)\n",
    "    s_new,r,d,_ = env.step(action)\n",
    "    total_reward += r\n",
    "    s = s_new\n",
    "    env.render()\n",
    "    print(\"step:\",j,\",r:\",r)\n",
    "print(\"total reward:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
