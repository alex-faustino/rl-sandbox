{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Reinforcement learning - Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classical control problem is heavily based on the already available acrobot example. The noticable differences between this implementation and the stock one is that the actions in this implementation is in the continuous space,a different reward function, some reorganisation to remove unnecessary functions, and the integrator from scipy replaces the hand coded RK45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym,sys\n",
    "import acrobot\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('vedant_acrobot-v0')\n",
    "\n",
    "act=np.array([-1.0,1.0])\n",
    "def run_episode(env, parameters):\n",
    "    action_list = []\n",
    "    reward_list= []\n",
    "    observation_list= []\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for i in range(500):\n",
    "        env.render()\n",
    "        action = act[np.random.randint(2)]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        #action_list = np.append(action_list, (action))\n",
    "        reward_list= np.append(reward_list, reward)\n",
    "        #observation_list= np.append(observation_list, (observation))\n",
    "        action_list.append(action)\n",
    "        observation_list.append(observation)\n",
    "    return totalreward, action_list, reward_list, observation_list\n",
    "parameters = np.random.rand(4) * 2 - 1\n",
    "reward, action_list, reward_list, observation_list = run_episode(env,parameters)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:running the cellabove should have opened a new window with a render of acrobot. Shown below are the plots for the reward function, actions chosen, and states observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "plot(np.array(reward_list));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.array(action_list));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.array(observation_list));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym,sys\n",
    "import acrobot\n",
    "\n",
    "import Agents.DQN_Agent as DQNA\n",
    "\n",
    "env = gym.make('vedant_acrobot-v0')\n",
    "\n",
    "\n",
    "#Q, rewards = \n",
    "r = DQNA.DQN_Agent(env,input_layer = 7,BATCH_SIZE = 100,GAMMA = 0.9,TARGET_UPDATE = 25,initial_epsilon = 1, \n",
    "              final_epsilon = 0.1,total_episodes = 1000, annealing_period = None,max_steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pyplot import plot\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "meanr = []\n",
    "print(len(r))\n",
    "for i in range(len(r)):\n",
    "    r[i] = np.array(r[i])\n",
    "    mean = np.mean(r[i])\n",
    "    meanr.append(mean)\n",
    "#print(r)\n",
    "    #figure(i)\n",
    "    plot(r[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(meanr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(r[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(r[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
