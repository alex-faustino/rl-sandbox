{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Reinforcement learning - Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO agent on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vedant/.local/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0\tMoving average score: -2694.18\t\n",
      "Ep 10\tMoving average score: -4124.66\t\n",
      "Ep 20\tMoving average score: -4470.86\t\n",
      "Ep 30\tMoving average score: -3938.37\t\n",
      "Ep 40\tMoving average score: -3827.72\t\n",
      "Ep 50\tMoving average score: -3665.83\t\n",
      "Ep 60\tMoving average score: -3781.12\t\n",
      "Ep 70\tMoving average score: -3369.38\t\n",
      "Ep 80\tMoving average score: -4074.55\t\n",
      "Ep 90\tMoving average score: -4081.57\t\n",
      "Ep 100\tMoving average score: -3761.09\t\n",
      "Ep 110\tMoving average score: -3195.93\t\n",
      "Ep 120\tMoving average score: -4079.81\t\n",
      "Ep 130\tMoving average score: -3433.29\t\n",
      "Ep 140\tMoving average score: -3494.48\t\n",
      "Ep 150\tMoving average score: -3377.59\t\n",
      "Ep 160\tMoving average score: -3027.70\t\n",
      "Ep 170\tMoving average score: -2423.30\t\n",
      "Ep 180\tMoving average score: -2569.09\t\n",
      "Ep 190\tMoving average score: -3150.71\t\n",
      "Ep 200\tMoving average score: -2789.63\t\n",
      "Ep 210\tMoving average score: -2845.36\t\n",
      "Ep 220\tMoving average score: -2147.27\t\n",
      "Ep 230\tMoving average score: -2282.21\t\n",
      "Ep 240\tMoving average score: -2789.99\t\n",
      "Ep 250\tMoving average score: -1636.31\t\n",
      "Ep 260\tMoving average score: -2038.29\t\n",
      "Ep 270\tMoving average score: -1.91\t\n",
      "Ep 280\tMoving average score: -250.63\t\n",
      "Ep 290\tMoving average score: -741.35\t\n",
      "Ep 300\tMoving average score: -744.58\t\n",
      "Ep 310\tMoving average score: -504.45\t\n",
      "Ep 320\tMoving average score: -759.55\t\n",
      "Ep 330\tMoving average score: -523.95\t\n",
      "Ep 340\tMoving average score: -127.12\t\n",
      "Ep 350\tMoving average score: -1010.35\t\n",
      "Ep 360\tMoving average score: -1514.14\t\n",
      "Ep 370\tMoving average score: -510.83\t\n",
      "Ep 380\tMoving average score: -641.00\t\n",
      "Ep 390\tMoving average score: -257.15\t\n",
      "Ep 400\tMoving average score: -128.49\t\n",
      "Ep 410\tMoving average score: -237.77\t\n",
      "Ep 420\tMoving average score: -132.16\t\n",
      "Ep 430\tMoving average score: -366.15\t\n",
      "Ep 440\tMoving average score: -132.44\t\n",
      "Ep 450\tMoving average score: -136.95\t\n",
      "Ep 460\tMoving average score: -125.79\t\n",
      "Ep 470\tMoving average score: -240.72\t\n",
      "Ep 480\tMoving average score: -265.40\t\n",
      "Ep 490\tMoving average score: -364.20\t\n",
      "Ep 500\tMoving average score: -354.57\t\n",
      "Ep 510\tMoving average score: -401.60\t\n",
      "Ep 520\tMoving average score: -131.36\t\n",
      "Ep 530\tMoving average score: -130.68\t\n",
      "Ep 540\tMoving average score: -244.46\t\n",
      "Ep 550\tMoving average score: -550.23\t\n",
      "Ep 560\tMoving average score: -1.17\t\n",
      "Ep 570\tMoving average score: -377.81\t\n",
      "Ep 580\tMoving average score: -362.15\t\n",
      "Ep 590\tMoving average score: -123.92\t\n",
      "Ep 600\tMoving average score: -249.45\t\n",
      "Ep 610\tMoving average score: -4.02\t\n",
      "Ep 620\tMoving average score: -127.49\t\n",
      "Ep 630\tMoving average score: -118.12\t\n",
      "Ep 640\tMoving average score: -120.04\t\n",
      "Ep 650\tMoving average score: -346.52\t\n",
      "Ep 660\tMoving average score: -130.10\t\n",
      "Ep 670\tMoving average score: -121.54\t\n",
      "Ep 680\tMoving average score: -132.39\t\n",
      "Ep 690\tMoving average score: -121.85\t\n",
      "Ep 700\tMoving average score: -2625.61\t\n",
      "Ep 710\tMoving average score: -232.90\t\n",
      "Ep 720\tMoving average score: -124.98\t\n",
      "Ep 730\tMoving average score: -130.48\t\n",
      "Ep 740\tMoving average score: -126.98\t\n",
      "Ep 750\tMoving average score: -260.64\t\n",
      "Ep 760\tMoving average score: -128.26\t\n",
      "Ep 770\tMoving average score: -124.61\t\n",
      "Ep 780\tMoving average score: -244.30\t\n",
      "Ep 790\tMoving average score: -126.71\t\n",
      "Ep 800\tMoving average score: -119.93\t\n",
      "Ep 810\tMoving average score: -253.24\t\n",
      "Ep 820\tMoving average score: -2.46\t\n",
      "Ep 830\tMoving average score: -126.26\t\n",
      "Ep 840\tMoving average score: -239.94\t\n",
      "Ep 850\tMoving average score: -5.26\t\n",
      "Ep 860\tMoving average score: -3.69\t\n",
      "Ep 870\tMoving average score: -121.30\t\n",
      "Ep 880\tMoving average score: -237.80\t\n",
      "Ep 890\tMoving average score: -126.67\t\n",
      "Ep 900\tMoving average score: -3.20\t\n",
      "Ep 910\tMoving average score: -126.56\t\n",
      "Ep 920\tMoving average score: -238.61\t\n",
      "Ep 930\tMoving average score: -331.68\t\n",
      "Ep 940\tMoving average score: -129.73\t\n",
      "Ep 950\tMoving average score: -242.11\t\n",
      "Ep 960\tMoving average score: -122.32\t\n",
      "Ep 970\tMoving average score: -10.09\t\n",
      "Ep 980\tMoving average score: -357.20\t\n",
      "Ep 990\tMoving average score: -244.15\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load PPO.py\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "gamma=0.99\n",
    "seed=0\n",
    "render=False\n",
    "log_interval=10\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
    "Transition = namedtuple('Transition', ['s', 'a', 'a_log_p', 'r', 's_'])\n",
    "\n",
    "inner_neuron = 50\n",
    "class ActorCriticNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ActorCriticNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, inner_neuron)\n",
    "        self.fc2 = nn.Linear(inner_neuron, inner_neuron)\n",
    "        self.fc3 = nn.Linear(inner_neuron, inner_neuron)\n",
    "        self.mu_head = nn.Linear(inner_neuron, 1)\n",
    "        self.sigma_head = nn.Linear(inner_neuron, 1)\n",
    "        self.v_head = nn.Linear(inner_neuron, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #if (inner_neuron>=10):\n",
    "        #    x = F.dropout(self.fc1(x),0.2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #if (inner_neuron>=10):\n",
    "        #    x = F.dropout(self.fc2(x),0.2)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #if (inner_neuron>=10):\n",
    "        #    x = F.dropout(self.fc3(x),0.2)\n",
    "        mu = 2.0 * F.tanh(self.mu_head(x))\n",
    "        sigma = F.softplus(self.sigma_head(x))\n",
    "        state_value = self.v_head(x)\n",
    "        return (mu, sigma,state_value)\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, inner_neuron)\n",
    "        self.fc2 = nn.Linear(inner_neuron, inner_neuron)\n",
    "        self.fc3 = nn.Linear(inner_neuron, inner_neuron)\n",
    "        self.v_head = nn.Linear(inner_neuron, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        state_value = self.v_head(x)\n",
    "        return state_value\n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    clip_param = 0.3\n",
    "    max_grad_norm = 0.5\n",
    "    ppo_epoch = 10\n",
    "    buffer_capacity, batch_size = 1000, 32\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_step = 0\n",
    "        #self.anet = ActorNet().float()\n",
    "        #self.cnet = CriticNet().float()\n",
    "        self.acnet = ActorCriticNet().float()\n",
    "        self.buffer = []\n",
    "        self.counter = 0\n",
    "\n",
    "        #self.optimizer_a = optim.Adam(self.anet.parameters(), lr=1e-4)\n",
    "        #self.optimizer_c = optim.Adam(self.cnet.parameters(), lr=3e-4)\n",
    "        self.optimizer_ac = optim.Adam(self.acnet.parameters(), lr=1e-4)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            (mu, sigma,_) = self.acnet(state)\n",
    "        dist = Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action)\n",
    "        action.clamp(-2.0, 2.0)\n",
    "        return action.item(), action_log_prob.item()\n",
    "\n",
    "    def get_value(self, state):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _,_,state_value = self.acnet(state)\n",
    "        return state_value.item()\n",
    "\n",
    "    def save_param(self):\n",
    "        torch.save(self.anet.state_dict(), 'param/ppo_anet_params.pkl')\n",
    "        torch.save(self.cnet.state_dict(), 'param/ppo_cnet_params.pkl')\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        self.counter += 1\n",
    "        return self.counter % self.buffer_capacity == 0\n",
    "\n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "        s = torch.tensor([t.s for t in self.buffer], dtype=torch.float)\n",
    "        a = torch.tensor([t.a for t in self.buffer], dtype=torch.float).view(-1, 1)\n",
    "        r = torch.tensor([t.r for t in self.buffer], dtype=torch.float).view(-1, 1)\n",
    "        s_ = torch.tensor([t.s_ for t in self.buffer], dtype=torch.float)\n",
    "\n",
    "        old_action_log_probs = torch.tensor(\n",
    "            [t.a_log_p for t in self.buffer], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        r = (r - r.mean()) / (r.std() + 1e-5)\n",
    "        with torch.no_grad():\n",
    "            _,_,temp3 = self.acnet(s_)\n",
    "            target_v = r + gamma * temp3\n",
    "\n",
    "        _,_,temp = self.acnet(s) \n",
    "        adv = (target_v - temp).detach()\n",
    "\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            for index in BatchSampler(\n",
    "                    SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n",
    "\n",
    "                (mu, sigma,_) = self.acnet(s[index])\n",
    "                dist = Normal(mu, sigma)\n",
    "                action_log_probs = dist.log_prob(a[index])\n",
    "                ratio = torch.exp(action_log_probs - old_action_log_probs[index])\n",
    "\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n",
    "                                    1.0 + self.clip_param) * adv[index]\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "                _,_,temp2 = self.acnet(s[index])\n",
    "                value_loss = F.smooth_l1_loss(temp2, target_v[index])\n",
    "                ac_loss = action_loss+value_loss\n",
    "                \n",
    "                self.optimizer_ac.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.acnet.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_ac.step()\n",
    "                '''\n",
    "                self.optimizer_a.zero_grad()\n",
    "                action_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.anet.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_a.step()\n",
    "\n",
    "                self.optimizer_c.zero_grad()\n",
    "                value_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_c.step()\n",
    "                '''\n",
    "\n",
    "        del self.buffer[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    env.seed(seed)\n",
    "\n",
    "    agent = Agent()\n",
    "\n",
    "    training_records = []\n",
    "    running_reward = -1000\n",
    "    state = env.reset()\n",
    "    for i_ep in range(1000):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(500):\n",
    "            action, action_log_prob = agent.select_action(state)\n",
    "            state_, reward, done, _ = env.step([action])\n",
    "            if render:\n",
    "                env.render()\n",
    "            if agent.store(Transition(state, action, action_log_prob, (reward + 8) / 8, state_)):\n",
    "                agent.update()\n",
    "            score += reward\n",
    "            state = state_\n",
    "\n",
    "        running_reward = running_reward * 0.0 + score * 1.0\n",
    "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
    "\n",
    "        if i_ep % log_interval == 0:\n",
    "            print('Ep {}\\tMoving average score: {:.2f}\\t'.format(i_ep, running_reward))\n",
    "        if running_reward > 500:\n",
    "            print(\"Solved! Moving average score is now {}!\".format(running_reward))\n",
    "            env.close()\n",
    "            #agent.save_param()\n",
    "            break\n",
    "\n",
    "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
    "    plt.title('PPO')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving averaged episode reward')\n",
    "    #plt.savefig(\"img/ppo.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aa018c9602c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mep\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_records\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_records\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PPO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Moving averaged episode reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#plt.savefig(\"img/ppo.png\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_records' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
    "plt.title('PPO')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Moving averaged episode reward')\n",
    "#plt.savefig(\"img/ppo.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
