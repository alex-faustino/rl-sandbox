{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='black'> HomeWork-6: Single Neural Network trained for both actor and critic\n",
    "Netid: sbhamid2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your goal this week is to implement PPO and apply it to the simple pendulum. The main reference for PPO is Schulman et al (2017).__\n",
    "\n",
    "__You can think of PPO as a natural extension of REINFORCE (which you implemented in HW4 and HW5), where we are now using a deep net both to describe the policy (actor) and the value function (critic).__\n",
    "\n",
    "__In particular, a key step will be to implement the loss function in Eq. (9) of Schulman et al (2017). The input to the deep net should be the continuous state (joint angle, joint velocity). The output from the deep net should be both the mean and variance of a normal distribution from which the continuous action (joint torque) is sampled - a so-called \"Gaussian policy\" - and the value of the state. You are free to drop the \"entropy bonus\" term from Eq. (9). Note that pytorch and tensorflow will compute the gradient of the loss function for you - there is no need to derive an expression for the gradient yourself, as you did in HW4 for the tabular policy.__\n",
    "\n",
    "__Your implementation should be designed so that it can be used with arbitrary continuous-state/continuous-action gym environments. In particular, you should aspire to apply your same algorithm to the acrobot (where the deep net would output the mean and covariance of a multi-variate normal distribution as well as the value), once you have successfully applied it to the simple pendulum.__\n",
    "\n",
    "__Follow the same pattern as with previous homework - choose hyperparameters so that the training converges, plot the learning rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ppo_singlenet as ppo\n",
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "a_seed = 0\n",
    "env.seed(a_seed)\n",
    "\n",
    "set_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pendulum v1 version\n",
    "set_params.update({'clip': 0.2, 'max_grad': 0.5, 'ppo_epoch': 10, 'mem_size': 1000, 'batch_size': 32, \\\n",
    "                   'gamma': 0.9, 'no_eps': 100, 'eps_len': 1000, 'env': env, 'lr': 1e-4})\n",
    "pendulum_v1 = ppo.Agent(env)\n",
    "rewards_v1 = pendulum_v1.train_model(set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pendulum v5 version -> Changing memory size\n",
    "set_params.update({'clip': 0.2, 'max_grad': 0.5, 'ppo_epoch': 10, 'mem_size': 500, 'batch_size': 32, \\\n",
    "                   'gamma': 0.9, 'no_eps': 100, 'eps_len': 1000, 'env': env, 'lr': 1e-4})\n",
    "pendulum_v5 = ppo.Agent(env)\n",
    "rewards_v5 = pendulum_v5.train_model(set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pendulum v3 version-> Changing learning rate\n",
    "set_params.update({'clip': 0.2, 'max_grad': 0.5, 'ppo_epoch': 10, 'mem_size': 1000, 'batch_size': 32, \\\n",
    "                   'gamma': 0.9, 'no_eps': 100, 'eps_len': 1000, 'env': env, 'lr': 1e-3})\n",
    "pendulum_v3 = ppo.Agent(env)\n",
    "rewards_v3 = pendulum_v3.train_model(set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pendulum v4 version -> Changing batch size\n",
    "set_params.update({'clip': 0.2, 'max_grad': 0.5, 'ppo_epoch': 10, 'mem_size': 1000, 'batch_size': 16, \\\n",
    "                   'gamma': 0.9, 'no_eps': 100, 'eps_len': 1000, 'env': env, 'lr': 1e-4})\n",
    "pendulum_v4 = ppo.Agent(env)\n",
    "rewards_v4 = pendulum_v4.train_model(set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pendulum v2 version -> Changing episode length\n",
    "set_params.update({'clip': 0.2, 'max_grad': 0.5, 'ppo_epoch': 10, 'mem_size': 1000, 'batch_size': 32, \\\n",
    "                   'gamma': 0.9, 'no_eps': 100, 'eps_len': 1500, 'env': env, 'lr': 1e-4})\n",
    "pendulum_v2 = ppo.Agent(env)\n",
    "rewards_v2 = pendulum_v2.train_model(set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(np.transpose(rewards_v1)[0], np.transpose(rewards_v1)[1]/(pendulum_v1.eps_len), 'b.')\n",
    "plt.plot(np.transpose(rewards_v1)[0], np.transpose(rewards_v1)[1]/(pendulum_v1.eps_len), 'b', label='default')\n",
    "\n",
    "#plt.plot(np.transpose(rewards_v2)[0], np.transpose(rewards_v2)[1]/(pendulum_v2.eps_len), 'r.')\n",
    "plt.plot(np.transpose(rewards_v2)[0], np.transpose(rewards_v2)[1]/(pendulum_v2.eps_len), 'r', label='mem_size')#\n",
    "\n",
    "#plt.plot(np.transpose(rewards_v3)[0], np.transpose(rewards_v3)[1]/(pendulum_v3.eps_len), 'g.')\n",
    "plt.plot(np.transpose(rewards_v3)[0], np.transpose(rewards_v3)[1]/(pendulum_v3.eps_len), 'g', label='lr')\n",
    "\n",
    "#plt.plot(np.transpose(rewards_v4)[0], np.transpose(rewards_v4)[1]/(pendulum_v4.eps_len), 'k.')\n",
    "plt.plot(np.transpose(rewards_v4)[0], np.transpose(rewards_v4)[1]/(pendulum_v4.eps_len), 'k', label='batch_size')\n",
    "\n",
    "#plt.plot(np.transpose(rewards_v5)[0], np.transpose(rewards_v5)[1]/(pendulum_v5.eps_len), 'm.')\n",
    "plt.plot(np.transpose(rewards_v5)[0], np.transpose(rewards_v5)[1]/(pendulum_v5.eps_len), 'm', label='eps_len')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_len = 400\n",
    "agent = pendulum_v3\n",
    "record_pos, record_action, rewards = agent.test_model(eps_len)\n",
    "print('Rewards attained: ', rewards)\n",
    "plt.figure()\n",
    "get_angle = np.arctan2(np.transpose(record_pos)[1], np.transpose(record_pos)[0])* 180 / np.pi\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(eps_len), get_angle, 'b')\n",
    "plt.plot(np.arange(eps_len), get_angle, 'b.')\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(eps_len), np.transpose(record_pos)[2], 'b')\n",
    "plt.plot(np.arange(eps_len), np.transpose(record_pos)[2], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
