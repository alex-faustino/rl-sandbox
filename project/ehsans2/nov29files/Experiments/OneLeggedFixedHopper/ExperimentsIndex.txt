N: 3
T: 32
K: 10
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 50
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+8)/8.,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100]),

Environment Maker: run_ppo_experiment(num_loops = 50, Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, 
                                                                                robot_urdf = robot_urdf)).ipython_display()

----------------------
N: 3
T: 32
K: 10
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 50
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+8)/8.,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100]),

Environment Maker: run_ppo_experiment(num_loops = 50, Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, 
                                                                                robot_urdf = robot_urdf)).ipython_display()

----------------------
N: 3
T: 32
K: 10
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 50
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+8)/8.,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100]),

Environment Maker: run_ppo_experiment(num_loops = 50, Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, 
                                                                                robot_urdf = robot_urdf)).ipython_display()

----------------------
N: 3
T: 32
K: 10
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 50
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+8)/8.,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100]),

Environment Maker: run_ppo_experiment(num_loops = 50, Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, 
                                                                                robot_urdf = robot_urdf)).ipython_display()

----------------------
N: 3
T: 32
K: 10
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                     reward_shaping = lambda r: r-0.75,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                     action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                     action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                     neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100]),

Environment Maker:                     Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 20000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: r-0.75,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.POSITION_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 20000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: r-0.75,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 1000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 1000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 1000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 1000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 1000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 2
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 4
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 20000
Batch size: 32
c_1: 4
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/2,

Action Sigma: None
Max episode length: 480
Learning rate: 0.0001
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 8
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 100000
Batch size: 32
c_1: 4
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+2)/4,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 4 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 8
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 300
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+5)/10,

Action Sigma: None
Max episode length: 480
Learning rate: 1e-05
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf,
                                                                    controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 8
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: (r+5)/10,

Action Sigma: None
Max episode length: 480
Learning rate: 0.0001
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf,
                                                                    controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL)).ipython_display()

----------------------
N: 8
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                         reward_shaping = lambda r: r/10,

Action Sigma: None
Max episode length: 480
Learning rate: 0.0001
Action mean transformation:                         action_mean_transformation = lambda proposed_act_mean: 2 * tanh(proposed_act_mean),

Action std transformation:                         action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                         neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200]),

Environment Maker:                         Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf,
                                                                    controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 1000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 50
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 10,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 1000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 50
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 10,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 1000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 50
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 10,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 1000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 480
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 10,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 1000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 480
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 10,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 1000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 480
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 10,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 480
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: softplus(proposed_act_std),

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 50
Learning rate: 0.0001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5. ) * 1,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 50
Learning rate: 0.0001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5. ) * 1,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 32
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 480
Learning rate: 0.0001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: 5 * tanh(proposed_act_mean),

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5. ) * 1,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+15)/30,

Action Sigma: None
Max episode length: 50
Learning rate: 0.0001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 20. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 1.,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 10000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: (r+30)/30,

Action Sigma: None
Max episode length: 50
Learning rate: 0.0001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 10. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 1.,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 30000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r/3,

Action Sigma: None
Max episode length: 100
Learning rate: 0.0001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean / 5.) * 10. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 1.,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[3,4], control_mode = p.TORQUE_CONTROL))

----------------------
N: 8
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 30000
Batch size: 32
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r/3,

Action Sigma: None
Max episode length: 100
Learning rate: 0.0001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean / 5.) * 10. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 1.,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[1,2], control_mode = p.TORQUE_CONTROL))

----------------------
N: 4
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 30000
Batch size: 8
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.1/3,

Action Sigma: None
Max episode length: 50
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean / 5.) * 3000. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 1.,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100, 200], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.01, robot_urdf = robot_urdf, t_res = 20,
                                                                        controllable_joints=[1,2], control_mode = p.TORQUE_CONTROL))

----------------------
N: 10
T: 10
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.2
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 200
Learning rate: 1e-05
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 60. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
N: 10
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 240
Learning rate: 1e-05
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 60. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
N: 10
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 240
Learning rate: 1e-05
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 60. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
N: 10
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 240
Learning rate: 1e-05
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 60. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
N: 10
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 240
Learning rate: 1e-05
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 60. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
N: 10
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 240
Learning rate: 1e-05
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 60. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda s_dim, out_dim: mean_std_val_net_v3(s_dim, out_dim, hidden_layers_units = [100], activation_fn = nn.Tanh),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
N: 1
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 240
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 2. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda *args, **kwargs: PPO.mean_std_val_net_v3(*args, **{**kwargs, **dict(hidden_layers_units = [100], activation_fn = nn.Tanh)}),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.VELOCITY_CONTROL))

----------------------
N: 1
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 240
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 2. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda *args, **kwargs: PPO.mean_std_val_net_v3(*args, **{**kwargs, **dict(hidden_layers_units = [100], activation_fn = nn.Tanh)}),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.0125, robot_urdf = robot_urdf, t_res = 3,
                                                                        controllable_joints=controllable_joints, control_mode = p.VELOCITY_CONTROL))

----------------------
N: 1
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 120
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 2. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda *args, **kwargs: PPO.mean_std_val_net_v3(*args, **{**kwargs, **dict(hidden_layers_units = [100], activation_fn = nn.Tanh)}),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.2, robot_urdf = robot_urdf, t_res = 48,
                                                                        controllable_joints=controllable_joints, control_mode = p.VELOCITY_CONTROL))

----------------------
N: 1
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 120
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 2. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda *args, **kwargs: PPO.mean_std_val_net_v3(*args, **{**kwargs, **dict(hidden_layers_units = [100], activation_fn = nn.Tanh)}),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.2, robot_urdf = robot_urdf, t_res = 48,
                                                                        controllable_joints=controllable_joints, control_mode = p.VELOCITY_CONTROL))

----------------------
N: 1
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 120
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 2. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda *args, **kwargs: PPO.mean_std_val_net_v3(*args, **{**kwargs, **dict(hidden_layers_units = [100], activation_fn = nn.Tanh)}),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.2, robot_urdf = robot_urdf, t_res = 48,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
N: 1
T: 12
K: 2
gamma: 0.9
lamda: 1
epsilon: 0.02
Number of loops: 30000
Batch size: 4
c_1: 1
c_2: 0
Seperate Value Network: True
Reward Shaping:                             reward_shaping = lambda r: r*0.01,

Action Sigma: None
Max episode length: 120
Learning rate: 0.001
Action mean transformation:                             action_mean_transformation = lambda proposed_act_mean: tanh(proposed_act_mean) * 2. ,

Action std transformation:                             action_std_transformation = lambda proposed_act_std: sigmoid( proposed_act_std / 5.) * 2,

Neural net maker:                             neural_net_maker = lambda *args, **kwargs: PPO.mean_std_val_net_v3(*args, **{**kwargs, **dict(hidden_layers_units = [100], activation_fn = nn.Tanh)}),

Environment Maker:                             Environment_Maker = lambda : robotBulletEnv(gui=False, time_step = 0.2, robot_urdf = robot_urdf, t_res = 48,
                                                                        controllable_joints=controllable_joints, control_mode = p.TORQUE_CONTROL))

----------------------
